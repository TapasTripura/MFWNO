{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It trains MFWNO on MF Darcy data (2D time-independent problem)\n",
    "### HF data size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce-T3Tu5zyE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "torch.cuda.empty_cache()\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "from timeit import default_timer\n",
    "from pytorch_wavelets import DWT, IDWT # (or import DWT, IDWT)\n",
    "from pytorch_wavelets import DTCWTForward, DTCWTInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JbWtxniSerT"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbC_I56c-bb3"
   },
   "source": [
    "# WNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R08jMzYL-kLX"
   },
   "outputs": [],
   "source": [
    "class WaveConv2dCwt(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, level, size, wavelet1, wavelet2):\n",
    "        super(WaveConv2dCwt, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Wavelet layer. It does DWT, linear transform, and Inverse dWT. \n",
    "        !! It is computationally expensive than the discrete \"WaveConv2d\" !!\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.level = level\n",
    "        self.wavelet_level1 = wavelet1\n",
    "        self.wavelet_level2 = wavelet2        \n",
    "        dummy_data = torch.randn( 1,1,*size ) \n",
    "        dwt_ = DTCWTForward(J=self.level, biort=self.wavelet_level1,\n",
    "                            qshift=self.wavelet_level2)\n",
    "        mode_data, mode_coef = dwt_(dummy_data)\n",
    "        self.modes1 = mode_data.shape[-2]\n",
    "        self.modes2 = mode_data.shape[-1]\n",
    "        self.modes21 = mode_coef[-1].shape[-3]\n",
    "        self.modes22 = mode_coef[-1].shape[-2]\n",
    "        \n",
    "        # Parameter initilization\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights0 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2))\n",
    "        self.weights15r = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights15c = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights45r = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights45c = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights75r = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights75c = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights105r = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights105c = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights135r = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights135c = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights165r = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "        self.weights165c = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes21, self.modes22))\n",
    "\n",
    "    # Convolution\n",
    "    def mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input parameters: \n",
    "        -----------------\n",
    "        x : tensor, shape-[Batch * Channel * x * y]\n",
    "        Output parameters: \n",
    "        ------------------\n",
    "        x : tensor, shape-[Batch * Channel * x * y]\n",
    "        \"\"\"        \n",
    "        # Compute dual tree continuous Wavelet coefficients \n",
    "        cwt = DTCWTForward(J=self.level, biort=self.wavelet_level1, qshift=self.wavelet_level2).to(x.device)\n",
    "        x_ft, x_coeff = cwt(x)\n",
    "        \n",
    "        out_ft = torch.zeros_like(x_ft, device= x.device)\n",
    "        out_coeff = [torch.zeros_like(coeffs, device= x.device) for coeffs in x_coeff]\n",
    "        \n",
    "        # Multiply the final approximate Wavelet modes\n",
    "        out_ft = self.mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights0)\n",
    "        # Multiply the final detailed wavelet coefficients        \n",
    "        out_coeff[-1][:,:,0,:,:,0] = self.mul2d(x_coeff[-1][:,:,0,:,:,0].clone(), self.weights15r)\n",
    "        out_coeff[-1][:,:,0,:,:,1] = self.mul2d(x_coeff[-1][:,:,0,:,:,1].clone(), self.weights15c)\n",
    "        out_coeff[-1][:,:,1,:,:,0] = self.mul2d(x_coeff[-1][:,:,1,:,:,0].clone(), self.weights45r)\n",
    "        out_coeff[-1][:,:,1,:,:,1] = self.mul2d(x_coeff[-1][:,:,1,:,:,1].clone(), self.weights45c)\n",
    "        out_coeff[-1][:,:,2,:,:,0] = self.mul2d(x_coeff[-1][:,:,2,:,:,0].clone(), self.weights75r)\n",
    "        out_coeff[-1][:,:,2,:,:,1] = self.mul2d(x_coeff[-1][:,:,2,:,:,1].clone(), self.weights75c)\n",
    "        out_coeff[-1][:,:,3,:,:,0] = self.mul2d(x_coeff[-1][:,:,3,:,:,0].clone(), self.weights105r)\n",
    "        out_coeff[-1][:,:,3,:,:,1] = self.mul2d(x_coeff[-1][:,:,3,:,:,1].clone(), self.weights105c)\n",
    "        out_coeff[-1][:,:,4,:,:,0] = self.mul2d(x_coeff[-1][:,:,4,:,:,0].clone(), self.weights135r)\n",
    "        out_coeff[-1][:,:,4,:,:,1] = self.mul2d(x_coeff[-1][:,:,4,:,:,1].clone(), self.weights135c)\n",
    "        out_coeff[-1][:,:,5,:,:,0] = self.mul2d(x_coeff[-1][:,:,5,:,:,0].clone(), self.weights165r)\n",
    "        out_coeff[-1][:,:,5,:,:,1] = self.mul2d(x_coeff[-1][:,:,5,:,:,1].clone(), self.weights165c)\n",
    "        \n",
    "        # Return to physical space        \n",
    "        icwt = DTCWTInverse(biort=self.wavelet_level1, qshift=self.wavelet_level2).to(x.device)\n",
    "        x = icwt((out_ft, out_coeff))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B32s7iltRJ6w"
   },
   "outputs": [],
   "source": [
    "class WNO2d_mf(nn.Module):\n",
    "    def __init__(self, width, level, size, wavelet, in_channel, grid_range):\n",
    "        super(WNO2d_mf, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The WNO network. It contains 4 layers of the Wavelet integral layer.\n",
    "        1. Lift the input using v(x) = self.fc0 .\n",
    "        2. 4 layers of the integral operators v(+1) = g(K(.) + W)(v).\n",
    "            W is defined by self.w_; K is defined by self.conv_.\n",
    "        3. Project the output of last layer using self.fc1 and self.fc2.\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.level = level\n",
    "        self.width = width\n",
    "        self.size = size\n",
    "        self.wavelet1 = wavelet[0]\n",
    "        self.wavelet2 = wavelet[1]\n",
    "        self.in_channel = in_channel\n",
    "        self.grid_range = grid_range \n",
    "        self.padding = 1\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.in_channel, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = WaveConv2dCwt(self.width, self.width, self.level, self.size, self.wavelet1, self.wavelet2)\n",
    "        self.conv1 = WaveConv2dCwt(self.width, self.width, self.level, self.size, self.wavelet1, self.wavelet2)\n",
    "        self.conv2 = WaveConv2dCwt(self.width, self.width, self.level, self.size, self.wavelet1, self.wavelet2)\n",
    "        self.conv3 = WaveConv2dCwt(self.width, self.width, self.level, self.size, self.wavelet1, self.wavelet2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding]) # do padding, if required\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding] # remove padding, when required\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        # The grid of the solution\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, self.grid_range[0], size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, self.grid_range[1], size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYbWdZMv-wLM"
   },
   "source": [
    "# Training and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "If-6jdj6RuEr"
   },
   "outputs": [],
   "source": [
    "ntrain = 10\n",
    "ntest = 40\n",
    "epochs = 500\n",
    "last_m = 600\n",
    "batch_size = 5\n",
    "\n",
    "n_total = ntrain + ntest\n",
    "learning_rate = 0.001\n",
    "\n",
    "step_size = 50\n",
    "gamma = 0.75\n",
    "\n",
    "wavelet = ['near_sym_a', 'qshift_a']  # wavelet basis function\n",
    "level = 2        # lavel of wavelet decomposition\n",
    "width = 64       # uplifting dimension\n",
    "grid_range = [1, 1]\n",
    "in_channel = 4\n",
    "\n",
    "r = 2\n",
    "h = int(((101 - 1)/r) + 1)\n",
    "s = h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVQq4kzcRcd3"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\" Read data \"\"\"\n",
    "PATH = 'data/Darcy_Triangular_FNO_multifid_hmax018_hmin016.mat'\n",
    "reader = MatReader(PATH)\n",
    "\n",
    "x_train = np.array(reader.read_field('boundCoeff')[:,::r,::r][:,:s,:s])\n",
    "y_train = np.array(reader.read_field('sol')[:,::r,::r][:,:s,:s])\n",
    "y_train_l = np.array(reader.read_field('lressol')[:,::r,::r][:,:s,:s])\n",
    "x_or_h = x_train[last_m-n_total:last_m].reshape((n_total,s,s,1))\n",
    "y_or_h = y_train[last_m-n_total:last_m]\n",
    "y_or_l = y_train_l[last_m-n_total:last_m].reshape((n_total,s,s,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_or_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0rQJiB1S0Hb"
   },
   "outputs": [],
   "source": [
    "x_mf = np.concatenate((x_or_h,y_or_l),axis=-1)\n",
    "y_mf = y_or_h - y_or_l.reshape((n_total,s,s))\n",
    "\n",
    "x_mf = torch.tensor( x_mf, dtype=torch.float ) \n",
    "y_mf = torch.tensor( y_mf, dtype=torch.float ) \n",
    "    \n",
    "generator = torch.Generator().manual_seed(453)\n",
    "dataset = torch.utils.data.random_split(torch.utils.data.TensorDataset(x_mf, y_mf),\n",
    "                                    [ntrain, ntest], generator=generator)\n",
    "train_dataset_mf, test_dataset_mf = dataset[0], dataset[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing datasets\n",
    "\n",
    "x_train_mf, y_train_mf = train_dataset_mf[:][0], train_dataset_mf[:][1]\n",
    "x_test_mf, y_test_mf = test_dataset_mf[:][0], test_dataset_mf[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_mf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rhzgcb4IRZaw"
   },
   "outputs": [],
   "source": [
    "x_normalizer_mf = UnitGaussianNormalizer(x_train_mf)\n",
    "x_train_mf = x_normalizer_mf.encode(x_train_mf)\n",
    "x_test_mf = x_normalizer_mf.encode(x_test_mf)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train_mf)\n",
    "y_train_mf = y_normalizer.encode(y_train_mf)\n",
    "\n",
    "train_loader_mf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_mf, y_train_mf),\n",
    "                                             batch_size=batch_size, shuffle=False)\n",
    "test_loader_mf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_mf, y_test_mf),\n",
    "                                             batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-31oJvWVHO-"
   },
   "source": [
    "# MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1657835406286,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "siHHykHz-2ND",
    "outputId": "d0e63ddb-019c-433a-87a6-a59e4a4de8e0"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\" The MD-WNO model definition \"\"\"\n",
    "model_mf = WNO2d_mf(width=width, level=level, size=[s,s], wavelet=wavelet,\n",
    "              in_channel=in_channel, grid_range=grid_range).to(device)\n",
    "print(count_params(model_mf))\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mf.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390130,
     "status": "ok",
     "timestamp": 1657835796404,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "RuP6QIZt-5Ag",
    "outputId": "23d80b43-7f85-4913-d5cb-7ca38f738d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-113, Time-0.6774, Train-MSE-0.0000, Train-L2-0.0530, Test-L2-0.3320\n",
      "Epoch-114, Time-0.6706, Train-MSE-0.0000, Train-L2-0.0526, Test-L2-0.3323\n",
      "Epoch-115, Time-0.6675, Train-MSE-0.0000, Train-L2-0.0523, Test-L2-0.3325\n",
      "Epoch-116, Time-0.6761, Train-MSE-0.0000, Train-L2-0.0520, Test-L2-0.3320\n",
      "Epoch-117, Time-0.6675, Train-MSE-0.0000, Train-L2-0.0517, Test-L2-0.3327\n",
      "Epoch-118, Time-0.6666, Train-MSE-0.0000, Train-L2-0.0515, Test-L2-0.3323\n",
      "Epoch-119, Time-0.6679, Train-MSE-0.0000, Train-L2-0.0513, Test-L2-0.3326\n",
      "Epoch-120, Time-0.6645, Train-MSE-0.0000, Train-L2-0.0510, Test-L2-0.3329\n",
      "Epoch-121, Time-0.6711, Train-MSE-0.0000, Train-L2-0.0507, Test-L2-0.3321\n",
      "Epoch-122, Time-0.6862, Train-MSE-0.0000, Train-L2-0.0506, Test-L2-0.3340\n",
      "Epoch-123, Time-0.6846, Train-MSE-0.0000, Train-L2-0.0509, Test-L2-0.3318\n",
      "Epoch-124, Time-0.6836, Train-MSE-0.0000, Train-L2-0.0514, Test-L2-0.3350\n",
      "Epoch-125, Time-0.6928, Train-MSE-0.0000, Train-L2-0.0518, Test-L2-0.3318\n",
      "Epoch-126, Time-0.7333, Train-MSE-0.0000, Train-L2-0.0518, Test-L2-0.3344\n",
      "Epoch-127, Time-0.6952, Train-MSE-0.0000, Train-L2-0.0513, Test-L2-0.3329\n",
      "Epoch-128, Time-0.6985, Train-MSE-0.0000, Train-L2-0.0511, Test-L2-0.3330\n",
      "Epoch-129, Time-0.6917, Train-MSE-0.0000, Train-L2-0.0502, Test-L2-0.3346\n",
      "Epoch-130, Time-0.7003, Train-MSE-0.0000, Train-L2-0.0501, Test-L2-0.3319\n",
      "Epoch-131, Time-0.6837, Train-MSE-0.0000, Train-L2-0.0496, Test-L2-0.3364\n",
      "Epoch-132, Time-0.6833, Train-MSE-0.0000, Train-L2-0.0495, Test-L2-0.3312\n",
      "Epoch-133, Time-0.6892, Train-MSE-0.0000, Train-L2-0.0492, Test-L2-0.3383\n",
      "Epoch-134, Time-0.6967, Train-MSE-0.0000, Train-L2-0.0494, Test-L2-0.3304\n",
      "Epoch-135, Time-0.6915, Train-MSE-0.0000, Train-L2-0.0500, Test-L2-0.3405\n",
      "Epoch-136, Time-0.6938, Train-MSE-0.0000, Train-L2-0.0506, Test-L2-0.3314\n",
      "Epoch-137, Time-0.6920, Train-MSE-0.0000, Train-L2-0.0505, Test-L2-0.3404\n",
      "Epoch-138, Time-0.7212, Train-MSE-0.0000, Train-L2-0.0498, Test-L2-0.3333\n",
      "Epoch-139, Time-0.7031, Train-MSE-0.0000, Train-L2-0.0488, Test-L2-0.3388\n",
      "Epoch-140, Time-0.7069, Train-MSE-0.0000, Train-L2-0.0481, Test-L2-0.3346\n",
      "Epoch-141, Time-0.7000, Train-MSE-0.0000, Train-L2-0.0473, Test-L2-0.3380\n",
      "Epoch-142, Time-0.7102, Train-MSE-0.0000, Train-L2-0.0467, Test-L2-0.3356\n",
      "Epoch-143, Time-0.6936, Train-MSE-0.0000, Train-L2-0.0461, Test-L2-0.3371\n",
      "Epoch-144, Time-0.6912, Train-MSE-0.0000, Train-L2-0.0455, Test-L2-0.3374\n",
      "Epoch-145, Time-0.7449, Train-MSE-0.0000, Train-L2-0.0450, Test-L2-0.3357\n",
      "Epoch-146, Time-0.7006, Train-MSE-0.0000, Train-L2-0.0446, Test-L2-0.3393\n",
      "Epoch-147, Time-0.6919, Train-MSE-0.0000, Train-L2-0.0444, Test-L2-0.3348\n",
      "Epoch-148, Time-0.6976, Train-MSE-0.0000, Train-L2-0.0444, Test-L2-0.3403\n",
      "Epoch-149, Time-0.6951, Train-MSE-0.0000, Train-L2-0.0448, Test-L2-0.3351\n",
      "Epoch-150, Time-0.6943, Train-MSE-0.0000, Train-L2-0.0439, Test-L2-0.3412\n",
      "Epoch-151, Time-0.7009, Train-MSE-0.0000, Train-L2-0.0426, Test-L2-0.3350\n",
      "Epoch-152, Time-0.7065, Train-MSE-0.0000, Train-L2-0.0423, Test-L2-0.3413\n",
      "Epoch-153, Time-0.6995, Train-MSE-0.0000, Train-L2-0.0419, Test-L2-0.3355\n",
      "Epoch-154, Time-0.6875, Train-MSE-0.0000, Train-L2-0.0413, Test-L2-0.3408\n",
      "Epoch-155, Time-0.7016, Train-MSE-0.0000, Train-L2-0.0408, Test-L2-0.3360\n",
      "Epoch-156, Time-0.7255, Train-MSE-0.0000, Train-L2-0.0404, Test-L2-0.3399\n",
      "Epoch-157, Time-0.7460, Train-MSE-0.0000, Train-L2-0.0400, Test-L2-0.3377\n",
      "Epoch-158, Time-0.7366, Train-MSE-0.0000, Train-L2-0.0396, Test-L2-0.3385\n",
      "Epoch-159, Time-0.7051, Train-MSE-0.0000, Train-L2-0.0393, Test-L2-0.3385\n",
      "Epoch-160, Time-0.7053, Train-MSE-0.0000, Train-L2-0.0390, Test-L2-0.3386\n",
      "Epoch-161, Time-0.6943, Train-MSE-0.0000, Train-L2-0.0388, Test-L2-0.3383\n",
      "Epoch-162, Time-0.7072, Train-MSE-0.0000, Train-L2-0.0385, Test-L2-0.3395\n",
      "Epoch-163, Time-0.7040, Train-MSE-0.0000, Train-L2-0.0383, Test-L2-0.3379\n",
      "Epoch-164, Time-0.7041, Train-MSE-0.0000, Train-L2-0.0381, Test-L2-0.3401\n",
      "Epoch-165, Time-0.6977, Train-MSE-0.0000, Train-L2-0.0380, Test-L2-0.3378\n",
      "Epoch-166, Time-0.6913, Train-MSE-0.0000, Train-L2-0.0380, Test-L2-0.3405\n",
      "Epoch-167, Time-0.7354, Train-MSE-0.0000, Train-L2-0.0381, Test-L2-0.3378\n",
      "Epoch-168, Time-0.7145, Train-MSE-0.0000, Train-L2-0.0382, Test-L2-0.3411\n",
      "Epoch-169, Time-0.7098, Train-MSE-0.0000, Train-L2-0.0383, Test-L2-0.3379\n",
      "Epoch-170, Time-0.7031, Train-MSE-0.0000, Train-L2-0.0384, Test-L2-0.3415\n",
      "Epoch-171, Time-0.7063, Train-MSE-0.0000, Train-L2-0.0385, Test-L2-0.3381\n",
      "Epoch-172, Time-0.6885, Train-MSE-0.0000, Train-L2-0.0385, Test-L2-0.3420\n",
      "Epoch-173, Time-0.6979, Train-MSE-0.0000, Train-L2-0.0385, Test-L2-0.3384\n",
      "Epoch-174, Time-0.6948, Train-MSE-0.0000, Train-L2-0.0384, Test-L2-0.3423\n",
      "Epoch-175, Time-0.7047, Train-MSE-0.0000, Train-L2-0.0382, Test-L2-0.3387\n",
      "Epoch-176, Time-0.7032, Train-MSE-0.0000, Train-L2-0.0380, Test-L2-0.3424\n",
      "Epoch-177, Time-0.7018, Train-MSE-0.0000, Train-L2-0.0379, Test-L2-0.3392\n",
      "Epoch-178, Time-0.7262, Train-MSE-0.0000, Train-L2-0.0377, Test-L2-0.3423\n",
      "Epoch-179, Time-0.7008, Train-MSE-0.0000, Train-L2-0.0375, Test-L2-0.3398\n",
      "Epoch-180, Time-0.6911, Train-MSE-0.0000, Train-L2-0.0373, Test-L2-0.3420\n",
      "Epoch-181, Time-0.7031, Train-MSE-0.0000, Train-L2-0.0372, Test-L2-0.3406\n",
      "Epoch-182, Time-0.7028, Train-MSE-0.0000, Train-L2-0.0369, Test-L2-0.3417\n",
      "Epoch-183, Time-0.6960, Train-MSE-0.0000, Train-L2-0.0366, Test-L2-0.3413\n",
      "Epoch-184, Time-0.7001, Train-MSE-0.0000, Train-L2-0.0363, Test-L2-0.3412\n",
      "Epoch-185, Time-0.6983, Train-MSE-0.0000, Train-L2-0.0360, Test-L2-0.3421\n",
      "Epoch-186, Time-0.6888, Train-MSE-0.0000, Train-L2-0.0357, Test-L2-0.3409\n",
      "Epoch-187, Time-0.6889, Train-MSE-0.0000, Train-L2-0.0355, Test-L2-0.3427\n",
      "Epoch-188, Time-0.7018, Train-MSE-0.0000, Train-L2-0.0355, Test-L2-0.3409\n",
      "Epoch-189, Time-0.6979, Train-MSE-0.0000, Train-L2-0.0354, Test-L2-0.3430\n",
      "Epoch-190, Time-0.6884, Train-MSE-0.0000, Train-L2-0.0355, Test-L2-0.3412\n",
      "Epoch-191, Time-0.6824, Train-MSE-0.0000, Train-L2-0.0356, Test-L2-0.3431\n",
      "Epoch-192, Time-0.7472, Train-MSE-0.0000, Train-L2-0.0356, Test-L2-0.3416\n",
      "Epoch-193, Time-0.7276, Train-MSE-0.0000, Train-L2-0.0357, Test-L2-0.3437\n",
      "Epoch-194, Time-0.7027, Train-MSE-0.0000, Train-L2-0.0356, Test-L2-0.3418\n",
      "Epoch-195, Time-0.7103, Train-MSE-0.0000, Train-L2-0.0357, Test-L2-0.3442\n",
      "Epoch-196, Time-0.7064, Train-MSE-0.0000, Train-L2-0.0355, Test-L2-0.3419\n",
      "Epoch-197, Time-0.7075, Train-MSE-0.0000, Train-L2-0.0356, Test-L2-0.3447\n",
      "Epoch-198, Time-0.6971, Train-MSE-0.0000, Train-L2-0.0356, Test-L2-0.3422\n",
      "Epoch-199, Time-0.7060, Train-MSE-0.0000, Train-L2-0.0356, Test-L2-0.3448\n",
      "Epoch-200, Time-0.7106, Train-MSE-0.0000, Train-L2-0.0351, Test-L2-0.3418\n",
      "Epoch-201, Time-0.7113, Train-MSE-0.0000, Train-L2-0.0339, Test-L2-0.3449\n",
      "Epoch-202, Time-0.7129, Train-MSE-0.0000, Train-L2-0.0332, Test-L2-0.3416\n",
      "Epoch-203, Time-0.7626, Train-MSE-0.0000, Train-L2-0.0330, Test-L2-0.3451\n",
      "Epoch-204, Time-0.7047, Train-MSE-0.0000, Train-L2-0.0327, Test-L2-0.3423\n",
      "Epoch-205, Time-0.7100, Train-MSE-0.0000, Train-L2-0.0324, Test-L2-0.3450\n",
      "Epoch-206, Time-0.7126, Train-MSE-0.0000, Train-L2-0.0322, Test-L2-0.3424\n",
      "Epoch-207, Time-0.7409, Train-MSE-0.0000, Train-L2-0.0319, Test-L2-0.3455\n",
      "Epoch-208, Time-0.7374, Train-MSE-0.0000, Train-L2-0.0318, Test-L2-0.3421\n",
      "Epoch-209, Time-0.7086, Train-MSE-0.0000, Train-L2-0.0317, Test-L2-0.3462\n",
      "Epoch-210, Time-0.7101, Train-MSE-0.0000, Train-L2-0.0316, Test-L2-0.3420\n",
      "Epoch-211, Time-0.7266, Train-MSE-0.0000, Train-L2-0.0315, Test-L2-0.3465\n",
      "Epoch-212, Time-0.7350, Train-MSE-0.0000, Train-L2-0.0314, Test-L2-0.3423\n",
      "Epoch-213, Time-0.7030, Train-MSE-0.0000, Train-L2-0.0311, Test-L2-0.3465\n",
      "Epoch-214, Time-0.7060, Train-MSE-0.0000, Train-L2-0.0310, Test-L2-0.3427\n",
      "Epoch-215, Time-0.7251, Train-MSE-0.0000, Train-L2-0.0308, Test-L2-0.3464\n",
      "Epoch-216, Time-0.7179, Train-MSE-0.0000, Train-L2-0.0307, Test-L2-0.3430\n",
      "Epoch-217, Time-0.7196, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.3465\n",
      "Epoch-218, Time-0.7323, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.3431\n",
      "Epoch-219, Time-0.7114, Train-MSE-0.0000, Train-L2-0.0305, Test-L2-0.3469\n",
      "Epoch-220, Time-0.7087, Train-MSE-0.0000, Train-L2-0.0305, Test-L2-0.3432\n",
      "Epoch-221, Time-0.7072, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.3473\n",
      "Epoch-222, Time-0.7225, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.3432\n",
      "Epoch-223, Time-0.7193, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.3478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-224, Time-0.7211, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.3432\n",
      "Epoch-225, Time-0.7074, Train-MSE-0.0000, Train-L2-0.0305, Test-L2-0.3480\n",
      "Epoch-226, Time-0.7032, Train-MSE-0.0000, Train-L2-0.0304, Test-L2-0.3434\n",
      "Epoch-227, Time-0.7136, Train-MSE-0.0000, Train-L2-0.0303, Test-L2-0.3481\n",
      "Epoch-228, Time-0.7175, Train-MSE-0.0000, Train-L2-0.0301, Test-L2-0.3437\n",
      "Epoch-229, Time-0.7491, Train-MSE-0.0000, Train-L2-0.0300, Test-L2-0.3482\n",
      "Epoch-230, Time-0.7072, Train-MSE-0.0000, Train-L2-0.0299, Test-L2-0.3440\n",
      "Epoch-231, Time-0.7008, Train-MSE-0.0000, Train-L2-0.0298, Test-L2-0.3482\n",
      "Epoch-232, Time-0.7084, Train-MSE-0.0000, Train-L2-0.0298, Test-L2-0.3443\n",
      "Epoch-233, Time-0.7044, Train-MSE-0.0000, Train-L2-0.0297, Test-L2-0.3484\n",
      "Epoch-234, Time-0.7149, Train-MSE-0.0000, Train-L2-0.0297, Test-L2-0.3445\n",
      "Epoch-235, Time-0.7183, Train-MSE-0.0000, Train-L2-0.0296, Test-L2-0.3486\n",
      "Epoch-236, Time-0.7102, Train-MSE-0.0000, Train-L2-0.0296, Test-L2-0.3448\n",
      "Epoch-237, Time-0.7092, Train-MSE-0.0000, Train-L2-0.0295, Test-L2-0.3486\n",
      "Epoch-238, Time-0.7149, Train-MSE-0.0000, Train-L2-0.0294, Test-L2-0.3453\n",
      "Epoch-239, Time-0.7074, Train-MSE-0.0000, Train-L2-0.0293, Test-L2-0.3485\n",
      "Epoch-240, Time-0.7176, Train-MSE-0.0000, Train-L2-0.0293, Test-L2-0.3457\n",
      "Epoch-241, Time-0.7480, Train-MSE-0.0000, Train-L2-0.0291, Test-L2-0.3485\n",
      "Epoch-242, Time-0.7140, Train-MSE-0.0000, Train-L2-0.0290, Test-L2-0.3461\n",
      "Epoch-243, Time-0.7025, Train-MSE-0.0000, Train-L2-0.0288, Test-L2-0.3484\n",
      "Epoch-244, Time-0.7118, Train-MSE-0.0000, Train-L2-0.0286, Test-L2-0.3463\n",
      "Epoch-245, Time-0.6976, Train-MSE-0.0000, Train-L2-0.0284, Test-L2-0.3485\n",
      "Epoch-246, Time-0.7082, Train-MSE-0.0000, Train-L2-0.0282, Test-L2-0.3464\n",
      "Epoch-247, Time-0.7055, Train-MSE-0.0000, Train-L2-0.0281, Test-L2-0.3488\n",
      "Epoch-248, Time-0.7353, Train-MSE-0.0000, Train-L2-0.0281, Test-L2-0.3464\n",
      "Epoch-249, Time-0.7026, Train-MSE-0.0000, Train-L2-0.0281, Test-L2-0.3493\n",
      "Epoch-250, Time-0.7592, Train-MSE-0.0000, Train-L2-0.0279, Test-L2-0.3462\n",
      "Epoch-251, Time-0.7053, Train-MSE-0.0000, Train-L2-0.0272, Test-L2-0.3491\n",
      "Epoch-252, Time-0.7146, Train-MSE-0.0000, Train-L2-0.0268, Test-L2-0.3469\n",
      "Epoch-253, Time-0.7152, Train-MSE-0.0000, Train-L2-0.0267, Test-L2-0.3485\n",
      "Epoch-254, Time-0.7067, Train-MSE-0.0000, Train-L2-0.0266, Test-L2-0.3478\n",
      "Epoch-255, Time-0.7177, Train-MSE-0.0000, Train-L2-0.0264, Test-L2-0.3480\n",
      "Epoch-256, Time-0.7215, Train-MSE-0.0000, Train-L2-0.0262, Test-L2-0.3483\n",
      "Epoch-257, Time-0.7078, Train-MSE-0.0000, Train-L2-0.0262, Test-L2-0.3477\n",
      "Epoch-258, Time-0.7331, Train-MSE-0.0000, Train-L2-0.0261, Test-L2-0.3488\n",
      "Epoch-259, Time-0.7213, Train-MSE-0.0000, Train-L2-0.0260, Test-L2-0.3476\n",
      "Epoch-260, Time-0.7108, Train-MSE-0.0000, Train-L2-0.0258, Test-L2-0.3488\n",
      "Epoch-261, Time-0.7063, Train-MSE-0.0000, Train-L2-0.0257, Test-L2-0.3479\n",
      "Epoch-262, Time-0.7115, Train-MSE-0.0000, Train-L2-0.0256, Test-L2-0.3487\n",
      "Epoch-263, Time-0.7642, Train-MSE-0.0000, Train-L2-0.0255, Test-L2-0.3481\n",
      "Epoch-264, Time-0.7199, Train-MSE-0.0000, Train-L2-0.0254, Test-L2-0.3487\n",
      "Epoch-265, Time-0.7040, Train-MSE-0.0000, Train-L2-0.0253, Test-L2-0.3483\n",
      "Epoch-266, Time-0.7001, Train-MSE-0.0000, Train-L2-0.0253, Test-L2-0.3487\n",
      "Epoch-267, Time-0.7135, Train-MSE-0.0000, Train-L2-0.0252, Test-L2-0.3484\n",
      "Epoch-268, Time-0.7082, Train-MSE-0.0000, Train-L2-0.0251, Test-L2-0.3489\n",
      "Epoch-269, Time-0.7167, Train-MSE-0.0000, Train-L2-0.0251, Test-L2-0.3484\n",
      "Epoch-270, Time-0.7569, Train-MSE-0.0000, Train-L2-0.0251, Test-L2-0.3491\n",
      "Epoch-271, Time-0.7216, Train-MSE-0.0000, Train-L2-0.0250, Test-L2-0.3484\n",
      "Epoch-272, Time-0.7066, Train-MSE-0.0000, Train-L2-0.0250, Test-L2-0.3492\n",
      "Epoch-273, Time-0.7143, Train-MSE-0.0000, Train-L2-0.0249, Test-L2-0.3484\n",
      "Epoch-274, Time-0.7211, Train-MSE-0.0000, Train-L2-0.0249, Test-L2-0.3494\n",
      "Epoch-275, Time-0.7339, Train-MSE-0.0000, Train-L2-0.0248, Test-L2-0.3484\n",
      "Epoch-276, Time-0.7269, Train-MSE-0.0000, Train-L2-0.0248, Test-L2-0.3496\n",
      "Epoch-277, Time-0.7393, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.3483\n",
      "Epoch-278, Time-0.7212, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.3498\n",
      "Epoch-279, Time-0.7226, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.3483\n",
      "Epoch-280, Time-0.7413, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.3502\n",
      "Epoch-281, Time-0.7325, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.3481\n",
      "Epoch-282, Time-0.7240, Train-MSE-0.0000, Train-L2-0.0248, Test-L2-0.3506\n",
      "Epoch-283, Time-0.7636, Train-MSE-0.0000, Train-L2-0.0248, Test-L2-0.3480\n",
      "Epoch-284, Time-0.7077, Train-MSE-0.0000, Train-L2-0.0249, Test-L2-0.3510\n",
      "Epoch-285, Time-0.7087, Train-MSE-0.0000, Train-L2-0.0249, Test-L2-0.3479\n",
      "Epoch-286, Time-0.7388, Train-MSE-0.0000, Train-L2-0.0249, Test-L2-0.3513\n",
      "Epoch-287, Time-0.7600, Train-MSE-0.0000, Train-L2-0.0249, Test-L2-0.3480\n",
      "Epoch-288, Time-0.7170, Train-MSE-0.0000, Train-L2-0.0248, Test-L2-0.3514\n",
      "Epoch-289, Time-0.7017, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.3482\n",
      "Epoch-290, Time-0.7150, Train-MSE-0.0000, Train-L2-0.0246, Test-L2-0.3515\n",
      "Epoch-291, Time-0.7522, Train-MSE-0.0000, Train-L2-0.0246, Test-L2-0.3483\n",
      "Epoch-292, Time-0.7124, Train-MSE-0.0000, Train-L2-0.0245, Test-L2-0.3516\n",
      "Epoch-293, Time-0.7209, Train-MSE-0.0000, Train-L2-0.0244, Test-L2-0.3485\n",
      "Epoch-294, Time-0.7214, Train-MSE-0.0000, Train-L2-0.0243, Test-L2-0.3517\n",
      "Epoch-295, Time-0.7572, Train-MSE-0.0000, Train-L2-0.0243, Test-L2-0.3487\n",
      "Epoch-296, Time-0.7211, Train-MSE-0.0000, Train-L2-0.0242, Test-L2-0.3518\n",
      "Epoch-297, Time-0.7255, Train-MSE-0.0000, Train-L2-0.0241, Test-L2-0.3488\n",
      "Epoch-298, Time-0.7019, Train-MSE-0.0000, Train-L2-0.0240, Test-L2-0.3519\n",
      "Epoch-299, Time-0.7132, Train-MSE-0.0000, Train-L2-0.0239, Test-L2-0.3489\n",
      "Epoch-300, Time-0.7717, Train-MSE-0.0000, Train-L2-0.0237, Test-L2-0.3518\n",
      "Epoch-301, Time-0.6998, Train-MSE-0.0000, Train-L2-0.0233, Test-L2-0.3497\n",
      "Epoch-302, Time-0.7029, Train-MSE-0.0000, Train-L2-0.0230, Test-L2-0.3504\n",
      "Epoch-303, Time-0.7082, Train-MSE-0.0000, Train-L2-0.0228, Test-L2-0.3511\n",
      "Epoch-304, Time-0.7008, Train-MSE-0.0000, Train-L2-0.0228, Test-L2-0.3496\n",
      "Epoch-305, Time-0.7450, Train-MSE-0.0000, Train-L2-0.0228, Test-L2-0.3517\n",
      "Epoch-306, Time-0.7102, Train-MSE-0.0000, Train-L2-0.0227, Test-L2-0.3498\n",
      "Epoch-307, Time-0.7126, Train-MSE-0.0000, Train-L2-0.0226, Test-L2-0.3512\n",
      "Epoch-308, Time-0.7051, Train-MSE-0.0000, Train-L2-0.0225, Test-L2-0.3505\n",
      "Epoch-309, Time-0.7425, Train-MSE-0.0000, Train-L2-0.0224, Test-L2-0.3507\n",
      "Epoch-310, Time-0.7116, Train-MSE-0.0000, Train-L2-0.0223, Test-L2-0.3511\n",
      "Epoch-311, Time-0.7145, Train-MSE-0.0000, Train-L2-0.0223, Test-L2-0.3504\n",
      "Epoch-312, Time-0.7013, Train-MSE-0.0000, Train-L2-0.0222, Test-L2-0.3513\n",
      "Epoch-313, Time-0.7478, Train-MSE-0.0000, Train-L2-0.0222, Test-L2-0.3505\n",
      "Epoch-314, Time-0.7046, Train-MSE-0.0000, Train-L2-0.0221, Test-L2-0.3513\n",
      "Epoch-315, Time-0.7000, Train-MSE-0.0000, Train-L2-0.0220, Test-L2-0.3508\n",
      "Epoch-316, Time-0.7521, Train-MSE-0.0000, Train-L2-0.0220, Test-L2-0.3511\n",
      "Epoch-317, Time-0.7181, Train-MSE-0.0000, Train-L2-0.0219, Test-L2-0.3510\n",
      "Epoch-318, Time-0.6972, Train-MSE-0.0000, Train-L2-0.0218, Test-L2-0.3510\n",
      "Epoch-319, Time-0.7136, Train-MSE-0.0000, Train-L2-0.0218, Test-L2-0.3513\n",
      "Epoch-320, Time-0.7249, Train-MSE-0.0000, Train-L2-0.0217, Test-L2-0.3510\n",
      "Epoch-321, Time-0.7042, Train-MSE-0.0000, Train-L2-0.0217, Test-L2-0.3515\n",
      "Epoch-322, Time-0.7126, Train-MSE-0.0000, Train-L2-0.0217, Test-L2-0.3509\n",
      "Epoch-323, Time-0.7138, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3516\n",
      "Epoch-324, Time-0.7349, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3510\n",
      "Epoch-325, Time-0.6965, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3517\n",
      "Epoch-326, Time-0.7047, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3510\n",
      "Epoch-327, Time-0.7227, Train-MSE-0.0000, Train-L2-0.0215, Test-L2-0.3518\n",
      "Epoch-328, Time-0.7223, Train-MSE-0.0000, Train-L2-0.0215, Test-L2-0.3511\n",
      "Epoch-329, Time-0.7207, Train-MSE-0.0000, Train-L2-0.0215, Test-L2-0.3518\n",
      "Epoch-330, Time-0.7074, Train-MSE-0.0000, Train-L2-0.0215, Test-L2-0.3511\n",
      "Epoch-331, Time-0.7067, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3519\n",
      "Epoch-332, Time-0.7410, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3512\n",
      "Epoch-333, Time-0.7757, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3520\n",
      "Epoch-334, Time-0.7164, Train-MSE-0.0000, Train-L2-0.0217, Test-L2-0.3513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-335, Time-0.7080, Train-MSE-0.0000, Train-L2-0.0218, Test-L2-0.3522\n",
      "Epoch-336, Time-0.7438, Train-MSE-0.0000, Train-L2-0.0219, Test-L2-0.3513\n",
      "Epoch-337, Time-0.7100, Train-MSE-0.0000, Train-L2-0.0219, Test-L2-0.3524\n",
      "Epoch-338, Time-0.7138, Train-MSE-0.0000, Train-L2-0.0220, Test-L2-0.3513\n",
      "Epoch-339, Time-0.7157, Train-MSE-0.0000, Train-L2-0.0221, Test-L2-0.3526\n",
      "Epoch-340, Time-0.7202, Train-MSE-0.0000, Train-L2-0.0221, Test-L2-0.3514\n",
      "Epoch-341, Time-0.7448, Train-MSE-0.0000, Train-L2-0.0220, Test-L2-0.3527\n",
      "Epoch-342, Time-0.7060, Train-MSE-0.0000, Train-L2-0.0219, Test-L2-0.3514\n",
      "Epoch-343, Time-0.7091, Train-MSE-0.0000, Train-L2-0.0218, Test-L2-0.3526\n",
      "Epoch-344, Time-0.7086, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.3515\n",
      "Epoch-345, Time-0.7830, Train-MSE-0.0000, Train-L2-0.0214, Test-L2-0.3526\n",
      "Epoch-346, Time-0.7441, Train-MSE-0.0000, Train-L2-0.0213, Test-L2-0.3517\n",
      "Epoch-347, Time-0.7356, Train-MSE-0.0000, Train-L2-0.0212, Test-L2-0.3526\n",
      "Epoch-348, Time-0.7606, Train-MSE-0.0000, Train-L2-0.0211, Test-L2-0.3518\n",
      "Epoch-349, Time-0.7013, Train-MSE-0.0000, Train-L2-0.0210, Test-L2-0.3526\n",
      "Epoch-350, Time-0.7026, Train-MSE-0.0000, Train-L2-0.0208, Test-L2-0.3518\n",
      "Epoch-351, Time-0.7159, Train-MSE-0.0000, Train-L2-0.0206, Test-L2-0.3525\n",
      "Epoch-352, Time-0.7215, Train-MSE-0.0000, Train-L2-0.0205, Test-L2-0.3521\n",
      "Epoch-353, Time-0.7141, Train-MSE-0.0000, Train-L2-0.0203, Test-L2-0.3523\n",
      "Epoch-354, Time-0.7068, Train-MSE-0.0000, Train-L2-0.0203, Test-L2-0.3524\n",
      "Epoch-355, Time-0.7049, Train-MSE-0.0000, Train-L2-0.0202, Test-L2-0.3522\n",
      "Epoch-356, Time-0.7750, Train-MSE-0.0000, Train-L2-0.0202, Test-L2-0.3526\n",
      "Epoch-357, Time-0.7476, Train-MSE-0.0000, Train-L2-0.0202, Test-L2-0.3522\n",
      "Epoch-358, Time-0.7085, Train-MSE-0.0000, Train-L2-0.0201, Test-L2-0.3527\n",
      "Epoch-359, Time-0.7133, Train-MSE-0.0000, Train-L2-0.0200, Test-L2-0.3523\n",
      "Epoch-360, Time-0.7041, Train-MSE-0.0000, Train-L2-0.0200, Test-L2-0.3525\n",
      "Epoch-361, Time-0.7117, Train-MSE-0.0000, Train-L2-0.0199, Test-L2-0.3526\n",
      "Epoch-362, Time-0.7150, Train-MSE-0.0000, Train-L2-0.0199, Test-L2-0.3524\n",
      "Epoch-363, Time-0.7174, Train-MSE-0.0000, Train-L2-0.0199, Test-L2-0.3527\n",
      "Epoch-364, Time-0.7097, Train-MSE-0.0000, Train-L2-0.0198, Test-L2-0.3524\n",
      "Epoch-365, Time-0.7084, Train-MSE-0.0000, Train-L2-0.0198, Test-L2-0.3528\n",
      "Epoch-366, Time-0.7026, Train-MSE-0.0000, Train-L2-0.0197, Test-L2-0.3526\n",
      "Epoch-367, Time-0.7142, Train-MSE-0.0000, Train-L2-0.0197, Test-L2-0.3527\n",
      "Epoch-368, Time-0.7182, Train-MSE-0.0000, Train-L2-0.0197, Test-L2-0.3527\n",
      "Epoch-369, Time-0.7306, Train-MSE-0.0000, Train-L2-0.0196, Test-L2-0.3527\n",
      "Epoch-370, Time-0.7122, Train-MSE-0.0000, Train-L2-0.0196, Test-L2-0.3528\n",
      "Epoch-371, Time-0.7175, Train-MSE-0.0000, Train-L2-0.0195, Test-L2-0.3528\n",
      "Epoch-372, Time-0.7146, Train-MSE-0.0000, Train-L2-0.0195, Test-L2-0.3529\n",
      "Epoch-373, Time-0.7398, Train-MSE-0.0000, Train-L2-0.0195, Test-L2-0.3528\n",
      "Epoch-374, Time-0.7216, Train-MSE-0.0000, Train-L2-0.0194, Test-L2-0.3530\n",
      "Epoch-375, Time-0.7171, Train-MSE-0.0000, Train-L2-0.0194, Test-L2-0.3529\n",
      "Epoch-376, Time-0.7117, Train-MSE-0.0000, Train-L2-0.0194, Test-L2-0.3530\n",
      "Epoch-377, Time-0.7120, Train-MSE-0.0000, Train-L2-0.0193, Test-L2-0.3529\n",
      "Epoch-378, Time-0.7240, Train-MSE-0.0000, Train-L2-0.0193, Test-L2-0.3530\n",
      "Epoch-379, Time-0.7208, Train-MSE-0.0000, Train-L2-0.0193, Test-L2-0.3530\n",
      "Epoch-380, Time-0.7276, Train-MSE-0.0000, Train-L2-0.0192, Test-L2-0.3531\n",
      "Epoch-381, Time-0.7916, Train-MSE-0.0000, Train-L2-0.0192, Test-L2-0.3531\n",
      "Epoch-382, Time-0.7251, Train-MSE-0.0000, Train-L2-0.0191, Test-L2-0.3531\n",
      "Epoch-383, Time-0.7026, Train-MSE-0.0000, Train-L2-0.0191, Test-L2-0.3531\n",
      "Epoch-384, Time-0.7113, Train-MSE-0.0000, Train-L2-0.0191, Test-L2-0.3531\n",
      "Epoch-385, Time-0.7304, Train-MSE-0.0000, Train-L2-0.0190, Test-L2-0.3532\n",
      "Epoch-386, Time-0.7537, Train-MSE-0.0000, Train-L2-0.0190, Test-L2-0.3531\n",
      "Epoch-387, Time-0.7153, Train-MSE-0.0000, Train-L2-0.0190, Test-L2-0.3533\n",
      "Epoch-388, Time-0.7368, Train-MSE-0.0000, Train-L2-0.0189, Test-L2-0.3532\n",
      "Epoch-389, Time-0.6956, Train-MSE-0.0000, Train-L2-0.0189, Test-L2-0.3533\n",
      "Epoch-390, Time-0.6821, Train-MSE-0.0000, Train-L2-0.0189, Test-L2-0.3533\n",
      "Epoch-391, Time-0.6931, Train-MSE-0.0000, Train-L2-0.0188, Test-L2-0.3534\n",
      "Epoch-392, Time-0.7501, Train-MSE-0.0000, Train-L2-0.0188, Test-L2-0.3533\n",
      "Epoch-393, Time-0.7063, Train-MSE-0.0000, Train-L2-0.0188, Test-L2-0.3535\n",
      "Epoch-394, Time-0.7535, Train-MSE-0.0000, Train-L2-0.0187, Test-L2-0.3534\n",
      "Epoch-395, Time-0.7287, Train-MSE-0.0000, Train-L2-0.0187, Test-L2-0.3535\n",
      "Epoch-396, Time-0.7054, Train-MSE-0.0000, Train-L2-0.0187, Test-L2-0.3534\n",
      "Epoch-397, Time-0.7167, Train-MSE-0.0000, Train-L2-0.0186, Test-L2-0.3535\n",
      "Epoch-398, Time-0.7137, Train-MSE-0.0000, Train-L2-0.0186, Test-L2-0.3535\n",
      "Epoch-399, Time-0.6971, Train-MSE-0.0000, Train-L2-0.0186, Test-L2-0.3536\n",
      "Epoch-400, Time-0.6988, Train-MSE-0.0000, Train-L2-0.0185, Test-L2-0.3536\n",
      "Epoch-401, Time-0.7043, Train-MSE-0.0000, Train-L2-0.0185, Test-L2-0.3536\n",
      "Epoch-402, Time-0.7397, Train-MSE-0.0000, Train-L2-0.0185, Test-L2-0.3536\n",
      "Epoch-403, Time-0.7376, Train-MSE-0.0000, Train-L2-0.0184, Test-L2-0.3536\n",
      "Epoch-404, Time-0.6862, Train-MSE-0.0000, Train-L2-0.0184, Test-L2-0.3537\n",
      "Epoch-405, Time-0.6998, Train-MSE-0.0000, Train-L2-0.0184, Test-L2-0.3537\n",
      "Epoch-406, Time-0.7070, Train-MSE-0.0000, Train-L2-0.0184, Test-L2-0.3537\n",
      "Epoch-407, Time-0.6982, Train-MSE-0.0000, Train-L2-0.0183, Test-L2-0.3537\n",
      "Epoch-408, Time-0.7115, Train-MSE-0.0000, Train-L2-0.0183, Test-L2-0.3537\n",
      "Epoch-409, Time-0.7486, Train-MSE-0.0000, Train-L2-0.0183, Test-L2-0.3538\n",
      "Epoch-410, Time-0.7221, Train-MSE-0.0000, Train-L2-0.0182, Test-L2-0.3538\n",
      "Epoch-411, Time-0.7307, Train-MSE-0.0000, Train-L2-0.0182, Test-L2-0.3538\n",
      "Epoch-412, Time-0.7058, Train-MSE-0.0000, Train-L2-0.0182, Test-L2-0.3538\n",
      "Epoch-413, Time-0.7374, Train-MSE-0.0000, Train-L2-0.0182, Test-L2-0.3538\n",
      "Epoch-414, Time-0.7965, Train-MSE-0.0000, Train-L2-0.0181, Test-L2-0.3539\n",
      "Epoch-415, Time-0.7658, Train-MSE-0.0000, Train-L2-0.0181, Test-L2-0.3539\n",
      "Epoch-416, Time-0.7634, Train-MSE-0.0000, Train-L2-0.0181, Test-L2-0.3539\n",
      "Epoch-417, Time-0.7691, Train-MSE-0.0000, Train-L2-0.0181, Test-L2-0.3539\n",
      "Epoch-418, Time-0.7319, Train-MSE-0.0000, Train-L2-0.0180, Test-L2-0.3540\n",
      "Epoch-419, Time-0.7426, Train-MSE-0.0000, Train-L2-0.0180, Test-L2-0.3540\n",
      "Epoch-420, Time-0.7611, Train-MSE-0.0000, Train-L2-0.0180, Test-L2-0.3540\n",
      "Epoch-421, Time-0.7331, Train-MSE-0.0000, Train-L2-0.0180, Test-L2-0.3540\n",
      "Epoch-422, Time-0.7924, Train-MSE-0.0000, Train-L2-0.0179, Test-L2-0.3541\n",
      "Epoch-423, Time-0.7220, Train-MSE-0.0000, Train-L2-0.0179, Test-L2-0.3541\n",
      "Epoch-424, Time-0.7936, Train-MSE-0.0000, Train-L2-0.0179, Test-L2-0.3541\n",
      "Epoch-425, Time-0.7720, Train-MSE-0.0000, Train-L2-0.0178, Test-L2-0.3541\n",
      "Epoch-426, Time-0.7588, Train-MSE-0.0000, Train-L2-0.0178, Test-L2-0.3541\n",
      "Epoch-427, Time-0.7434, Train-MSE-0.0000, Train-L2-0.0178, Test-L2-0.3542\n",
      "Epoch-428, Time-0.7173, Train-MSE-0.0000, Train-L2-0.0178, Test-L2-0.3542\n",
      "Epoch-429, Time-0.7357, Train-MSE-0.0000, Train-L2-0.0177, Test-L2-0.3542\n",
      "Epoch-430, Time-0.7077, Train-MSE-0.0000, Train-L2-0.0177, Test-L2-0.3543\n",
      "Epoch-431, Time-0.7927, Train-MSE-0.0000, Train-L2-0.0177, Test-L2-0.3543\n",
      "Epoch-432, Time-0.7379, Train-MSE-0.0000, Train-L2-0.0177, Test-L2-0.3543\n",
      "Epoch-433, Time-0.7181, Train-MSE-0.0000, Train-L2-0.0176, Test-L2-0.3543\n",
      "Epoch-434, Time-0.7305, Train-MSE-0.0000, Train-L2-0.0176, Test-L2-0.3544\n",
      "Epoch-435, Time-0.7337, Train-MSE-0.0000, Train-L2-0.0176, Test-L2-0.3543\n",
      "Epoch-436, Time-0.6997, Train-MSE-0.0000, Train-L2-0.0176, Test-L2-0.3544\n",
      "Epoch-437, Time-0.7163, Train-MSE-0.0000, Train-L2-0.0175, Test-L2-0.3544\n",
      "Epoch-438, Time-0.7737, Train-MSE-0.0000, Train-L2-0.0175, Test-L2-0.3545\n",
      "Epoch-439, Time-0.7220, Train-MSE-0.0000, Train-L2-0.0175, Test-L2-0.3544\n",
      "Epoch-440, Time-0.8070, Train-MSE-0.0000, Train-L2-0.0174, Test-L2-0.3545\n",
      "Epoch-441, Time-0.7366, Train-MSE-0.0000, Train-L2-0.0174, Test-L2-0.3545\n",
      "Epoch-442, Time-0.7160, Train-MSE-0.0000, Train-L2-0.0174, Test-L2-0.3545\n",
      "Epoch-443, Time-0.7476, Train-MSE-0.0000, Train-L2-0.0174, Test-L2-0.3545\n",
      "Epoch-444, Time-0.7280, Train-MSE-0.0000, Train-L2-0.0173, Test-L2-0.3546\n",
      "Epoch-445, Time-0.7261, Train-MSE-0.0000, Train-L2-0.0173, Test-L2-0.3546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-446, Time-0.7596, Train-MSE-0.0000, Train-L2-0.0173, Test-L2-0.3546\n",
      "Epoch-447, Time-0.7689, Train-MSE-0.0000, Train-L2-0.0173, Test-L2-0.3546\n",
      "Epoch-448, Time-0.7237, Train-MSE-0.0000, Train-L2-0.0172, Test-L2-0.3547\n",
      "Epoch-449, Time-0.7143, Train-MSE-0.0000, Train-L2-0.0172, Test-L2-0.3546\n",
      "Epoch-450, Time-0.7423, Train-MSE-0.0000, Train-L2-0.0172, Test-L2-0.3547\n",
      "Epoch-451, Time-0.7361, Train-MSE-0.0000, Train-L2-0.0172, Test-L2-0.3547\n",
      "Epoch-452, Time-0.7495, Train-MSE-0.0000, Train-L2-0.0171, Test-L2-0.3547\n",
      "Epoch-453, Time-0.7023, Train-MSE-0.0000, Train-L2-0.0171, Test-L2-0.3548\n",
      "Epoch-454, Time-0.6963, Train-MSE-0.0000, Train-L2-0.0171, Test-L2-0.3547\n",
      "Epoch-455, Time-0.7624, Train-MSE-0.0000, Train-L2-0.0171, Test-L2-0.3548\n",
      "Epoch-456, Time-0.7298, Train-MSE-0.0000, Train-L2-0.0171, Test-L2-0.3548\n",
      "Epoch-457, Time-0.7382, Train-MSE-0.0000, Train-L2-0.0170, Test-L2-0.3548\n",
      "Epoch-458, Time-0.7200, Train-MSE-0.0000, Train-L2-0.0170, Test-L2-0.3548\n",
      "Epoch-459, Time-0.7317, Train-MSE-0.0000, Train-L2-0.0170, Test-L2-0.3548\n",
      "Epoch-460, Time-0.7335, Train-MSE-0.0000, Train-L2-0.0170, Test-L2-0.3549\n",
      "Epoch-461, Time-0.7383, Train-MSE-0.0000, Train-L2-0.0170, Test-L2-0.3549\n",
      "Epoch-462, Time-0.7399, Train-MSE-0.0000, Train-L2-0.0169, Test-L2-0.3549\n",
      "Epoch-463, Time-0.7377, Train-MSE-0.0000, Train-L2-0.0169, Test-L2-0.3549\n",
      "Epoch-464, Time-0.7394, Train-MSE-0.0000, Train-L2-0.0169, Test-L2-0.3549\n",
      "Epoch-465, Time-0.7118, Train-MSE-0.0000, Train-L2-0.0169, Test-L2-0.3549\n",
      "Epoch-466, Time-0.7234, Train-MSE-0.0000, Train-L2-0.0169, Test-L2-0.3550\n",
      "Epoch-467, Time-0.7652, Train-MSE-0.0000, Train-L2-0.0168, Test-L2-0.3550\n",
      "Epoch-468, Time-0.7511, Train-MSE-0.0000, Train-L2-0.0168, Test-L2-0.3550\n",
      "Epoch-469, Time-0.7528, Train-MSE-0.0000, Train-L2-0.0168, Test-L2-0.3550\n",
      "Epoch-470, Time-0.7252, Train-MSE-0.0000, Train-L2-0.0168, Test-L2-0.3551\n",
      "Epoch-471, Time-0.7713, Train-MSE-0.0000, Train-L2-0.0168, Test-L2-0.3551\n",
      "Epoch-472, Time-0.7375, Train-MSE-0.0000, Train-L2-0.0167, Test-L2-0.3551\n",
      "Epoch-473, Time-0.7077, Train-MSE-0.0000, Train-L2-0.0167, Test-L2-0.3551\n",
      "Epoch-474, Time-0.7358, Train-MSE-0.0000, Train-L2-0.0167, Test-L2-0.3551\n",
      "Epoch-475, Time-0.7075, Train-MSE-0.0000, Train-L2-0.0167, Test-L2-0.3551\n",
      "Epoch-476, Time-0.7821, Train-MSE-0.0000, Train-L2-0.0167, Test-L2-0.3551\n",
      "Epoch-477, Time-0.7511, Train-MSE-0.0000, Train-L2-0.0166, Test-L2-0.3552\n",
      "Epoch-478, Time-0.7313, Train-MSE-0.0000, Train-L2-0.0166, Test-L2-0.3552\n",
      "Epoch-479, Time-0.7306, Train-MSE-0.0000, Train-L2-0.0166, Test-L2-0.3552\n",
      "Epoch-480, Time-0.7252, Train-MSE-0.0000, Train-L2-0.0166, Test-L2-0.3552\n",
      "Epoch-481, Time-0.7224, Train-MSE-0.0000, Train-L2-0.0166, Test-L2-0.3552\n",
      "Epoch-482, Time-0.7205, Train-MSE-0.0000, Train-L2-0.0165, Test-L2-0.3553\n",
      "Epoch-483, Time-0.7503, Train-MSE-0.0000, Train-L2-0.0165, Test-L2-0.3553\n",
      "Epoch-484, Time-0.7695, Train-MSE-0.0000, Train-L2-0.0165, Test-L2-0.3553\n",
      "Epoch-485, Time-0.7200, Train-MSE-0.0000, Train-L2-0.0165, Test-L2-0.3553\n",
      "Epoch-486, Time-0.7254, Train-MSE-0.0000, Train-L2-0.0165, Test-L2-0.3554\n",
      "Epoch-487, Time-0.7237, Train-MSE-0.0000, Train-L2-0.0164, Test-L2-0.3553\n",
      "Epoch-488, Time-0.7127, Train-MSE-0.0000, Train-L2-0.0164, Test-L2-0.3554\n",
      "Epoch-489, Time-0.7140, Train-MSE-0.0000, Train-L2-0.0164, Test-L2-0.3554\n",
      "Epoch-490, Time-0.8213, Train-MSE-0.0000, Train-L2-0.0164, Test-L2-0.3554\n",
      "Epoch-491, Time-0.7197, Train-MSE-0.0000, Train-L2-0.0164, Test-L2-0.3555\n",
      "Epoch-492, Time-0.7558, Train-MSE-0.0000, Train-L2-0.0163, Test-L2-0.3554\n",
      "Epoch-493, Time-0.7087, Train-MSE-0.0000, Train-L2-0.0163, Test-L2-0.3555\n",
      "Epoch-494, Time-0.7328, Train-MSE-0.0000, Train-L2-0.0163, Test-L2-0.3555\n",
      "Epoch-495, Time-0.7991, Train-MSE-0.0000, Train-L2-0.0163, Test-L2-0.3555\n",
      "Epoch-496, Time-0.7240, Train-MSE-0.0000, Train-L2-0.0163, Test-L2-0.3555\n",
      "Epoch-497, Time-0.7475, Train-MSE-0.0000, Train-L2-0.0162, Test-L2-0.3556\n",
      "Epoch-498, Time-0.7506, Train-MSE-0.0000, Train-L2-0.0162, Test-L2-0.3556\n",
      "Epoch-499, Time-0.8013, Train-MSE-0.0000, Train-L2-0.0162, Test-L2-0.3556\n"
     ]
    }
   ],
   "source": [
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.to(device)\n",
    "for ep in range(epochs):\n",
    "    model_mf.train()\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader_mf:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_mf(x).reshape(x.shape[0], s, s)\n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "        \n",
    "        mse = F.mse_loss(out.view(x.shape[0], -1), y.view(x.shape[0], -1), reduction='mean')\n",
    "        loss = myloss(out.view(x.shape[0], -1), y.view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_mse += mse.item()\n",
    "        train_l2 += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    model_mf.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader_mf:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out = model_mf(x).reshape(x.shape[0], s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(x.shape[0], -1), y.view(x.shape[0], -1)).item()\n",
    "\n",
    "    train_mse /= len(train_loader_mf)\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "    t2 = default_timer()\n",
    "    print('Epoch-{}, Time-{:0.4f}, Train-MSE-{:0.4f}, Train-L2-{:0.4f}, Test-L2-{:0.4f}'\n",
    "          .format(ep, t2-t1, train_mse, train_l2, test_l2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MF-WNO model\n",
    "\n",
    "torch.save(model_mf, 'model/MF_WNO_Darcy2D_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCujdZ_PT4Vl"
   },
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "pred_mf = [] \n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    for x, y in test_loader_mf:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        out = model_mf(x).reshape(x.shape[0], s, s)\n",
    "        out = y_normalizer.decode(out)\n",
    "        test_l2 = myloss(out.view(x.shape[0], -1), y.view(x.shape[0], -1)).item()\n",
    "        \n",
    "        test_l2 /= x.shape[0]\n",
    "        print('Batch-{}, Test-L2-{:0.4f}'.format(index, test_l2))\n",
    "        \n",
    "        pred_mf.append(out.cpu())\n",
    "        index += 1\n",
    "\n",
    "pred_mf = torch.cat(( pred_mf ), dim=0 )\n",
    "\n",
    "print('Mean mse_hf-{}'.format(F.mse_loss(y_test_mf, pred_mf).item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYFr3qYG_Rge"
   },
   "outputs": [],
   "source": [
    "# Add the residual operator to LF-dataset \n",
    "input_mf = x_normalizer_mf.decode( x_test_mf.cpu() ) \n",
    "\n",
    "real_mf = y_test_mf + input_mf[..., 1]\n",
    "output_mf = pred_mf + input_mf[..., 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_mf.shape, output_mf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu0WJf6wT8lu"
   },
   "outputs": [],
   "source": [
    "mse_pred = F.mse_loss(output_mf, real_mf).item()\n",
    "mse_LF = F.mse_loss(real_mf, x_test_mf[..., 1])\n",
    "mse_residual = F.mse_loss(y_test_mf, pred_mf)\n",
    "\n",
    "print('MSE-Predicted solution-{:0.4f}, MSE-LF Data-{:0.4f}, MSE-Residual-{:0.4f}'\n",
    "      .format(mse_pred, mse_LF, mse_residual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6310,
     "status": "ok",
     "timestamp": 1657835802701,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "T6UkMuPyUTPP",
    "outputId": "b7cbb499-c02a-4be7-8c23-b8d19f64ba05"
   },
   "outputs": [],
   "source": [
    "fig1, axs = plt.subplots(nrows=3, ncols=5, figsize=(16, 6), facecolor='w', edgecolor='k')\n",
    "fig1.subplots_adjust(hspace=0.35, wspace=0.2)\n",
    "\n",
    "fig1.suptitle(f'Predictions MFWNO AC2d Size', fontsize=16)\n",
    "index = 0 \n",
    "for sample in range(ntest):\n",
    "    if sample % 9 == 0:\n",
    "        im = axs[0, index].imshow(real_mf[sample, :, :], cmap='nipy_spectral', origin='lower' )\n",
    "        plt.colorbar(im, ax=axs[0, index])\n",
    "        im = axs[1, index].imshow(output_mf[sample, :, :], cmap='nipy_spectral', origin='lower' )\n",
    "        plt.colorbar(im, ax=axs[1, index])\n",
    "        im = axs[2, index].imshow(torch.abs(real_mf[sample, :, :] - output_mf[sample, :, :]),\n",
    "                                    cmap='jet', origin='lower')\n",
    "        plt.colorbar(im, ax=axs[2, index])\n",
    "        index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ElUfPiIXy8s"
   },
   "source": [
    "# High Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, level, size, wavelet):\n",
    "        super(WaveConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Wavelet layer. It does DWT, linear transform, and Inverse dWT. \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.level = level\n",
    "        self.wavelet = wavelet       \n",
    "        dummy_data = torch.randn( 1,1,*size )        \n",
    "        dwt_ = DWT(J=self.level, mode='symmetric', wave=self.wavelet)\n",
    "        mode_data, mode_coef = dwt_(dummy_data)\n",
    "        self.modes1 = mode_data.shape[-2]\n",
    "        self.modes2 = mode_data.shape[-1]\n",
    "        \n",
    "        # Parameter initilization\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2))\n",
    "        self.weights3 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2))\n",
    "        self.weights4 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2))\n",
    "\n",
    "    # Convolution\n",
    "    def mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input parameters: \n",
    "        -----------------\n",
    "        x : tensor, shape-[Batch * Channel * x * y]\n",
    "        Output parameters: \n",
    "        ------------------\n",
    "        x : tensor, shape-[Batch * Channel * x * y]\n",
    "        \"\"\"\n",
    "        # Compute single tree Discrete Wavelet coefficients using some wavelet\n",
    "        dwt = DWT(J=self.level, mode='symmetric', wave=self.wavelet).to(x.device)\n",
    "        x_ft, x_coeff = dwt(x)\n",
    "\n",
    "        # Multiply the final approximate Wavelet modes\n",
    "        out_ft = self.mul2d(x_ft, self.weights1)\n",
    "        # Multiply the final detailed wavelet coefficients\n",
    "        x_coeff[-1][:,:,0,:,:] = self.mul2d(x_coeff[-1][:,:,0,:,:].clone(), self.weights2)\n",
    "        x_coeff[-1][:,:,1,:,:] = self.mul2d(x_coeff[-1][:,:,1,:,:].clone(), self.weights3)\n",
    "        x_coeff[-1][:,:,2,:,:] = self.mul2d(x_coeff[-1][:,:,2,:,:].clone(), self.weights4)\n",
    "        \n",
    "        # Return to physical space        \n",
    "        idwt = IDWT(mode='symmetric', wave=self.wavelet).to(x.device)\n",
    "        x = idwt((out_ft, x_coeff))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpeJTDt3-kyO"
   },
   "outputs": [],
   "source": [
    "class WNO2d(nn.Module):\n",
    "    def __init__(self, width, level, size, wavelet, in_channel, grid_range):\n",
    "        super(WNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The WNO network. It contains 4 layers of the Wavelet integral layer.\n",
    "        1. Lift the input using v(x) = self.fc0 .\n",
    "        2. 4 layers of the integral operators v(+1) = g(K(.) + W)(v).\n",
    "            W is defined by self.w_; K is defined by self.conv_.\n",
    "        3. Project the output of last layer using self.fc1 and self.fc2.\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.level = level\n",
    "        self.width = width\n",
    "        self.size = size\n",
    "        self.wavelet = wavelet\n",
    "        self.in_channel = in_channel\n",
    "        self.grid_range = grid_range \n",
    "        self.padding = 1\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.in_channel, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = WaveConv2d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv1 = WaveConv2d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv2 = WaveConv2d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv3 = WaveConv2d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding]) # do padding, if required\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding] # remove padding, when required\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        # The grid of the solution\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, self.grid_range[0], size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, self.grid_range[1], size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHcYiiunVyvk"
   },
   "outputs": [],
   "source": [
    "ntrain = ntrain\n",
    "ntest = ntest\n",
    "n_total = ntrain + ntest\n",
    "batch_size = batch_size\n",
    "learning_rate = 0.001\n",
    "\n",
    "wavelet = 'db6'  # wavelet basis function\n",
    "level = 2        # lavel of wavelet decomposition\n",
    "width = 64       # uplifting dimension\n",
    "grid_range = [1, 1]\n",
    "in_channel = 3\n",
    "\n",
    "epochs = 250\n",
    "step_size = 50\n",
    "gamma = 0.75\n",
    "\n",
    "r = 2\n",
    "h = int(((101 - 1)/r) + 1)\n",
    "s = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L-g7RCfWJ6q"
   },
   "outputs": [],
   "source": [
    "# Create the input and output (residual) dataset\n",
    "x_hf = torch.tensor( x_or_h, dtype=torch.float ) \n",
    "y_hf = torch.tensor( y_or_h, dtype=torch.float ) \n",
    "    \n",
    "generator_hf = torch.Generator().manual_seed(453)\n",
    "dataset_hf = torch.utils.data.random_split(torch.utils.data.TensorDataset(x_hf, y_hf),\n",
    "                                    [ntrain, ntest], generator=generator)\n",
    "train_data_hf, test_data_hf = dataset_hf[0], dataset_hf[1]\n",
    "\n",
    "# Split the training and testing datasets\n",
    "x_train_hf, y_train_hf = train_data_hf[:][0], train_data_hf[:][1]\n",
    "x_test_hf, y_test_hf = test_data_hf[:][0], test_data_hf[:][1]\n",
    "\n",
    "x_normalizer_hf = UnitGaussianNormalizer(x_train_hf)\n",
    "x_train_hf = x_normalizer_hf.encode(x_train_hf)\n",
    "x_test_hf = x_normalizer_hf.encode(x_test_hf)\n",
    "\n",
    "y_normalizer_hf = UnitGaussianNormalizer(y_train_hf)\n",
    "y_train_hf = y_normalizer_hf.encode(y_train_hf)\n",
    "\n",
    "# Define the dataloaders\n",
    "train_loader_hf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_hf, y_train_hf),\n",
    "                                             batch_size=batch_size, shuffle=True)\n",
    "test_loader_hf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_hf, y_test_hf),\n",
    "                                            batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1657835825198,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "IT3OsJelW1S7",
    "outputId": "9bc0e6c9-7dfe-4728-90c3-9755ec8255fc"
   },
   "outputs": [],
   "source": [
    "model = WNO2d(width=width, level=level, size=[s,s], wavelet=wavelet,\n",
    "              in_channel=in_channel, grid_range=grid_range).to(device)\n",
    "print(count_params(model))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390472,
     "status": "ok",
     "timestamp": 1657836215652,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "tjg6qNVzW9Tb",
    "outputId": "3dc6fbe2-08f8-4b39-d814-407b7a655350",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-113, Time-0.4512, Train-MSE-0.0097, Train-L2-0.1003, Test-L2-0.8076\n",
      "Epoch-114, Time-0.4427, Train-MSE-0.0111, Train-L2-0.1183, Test-L2-0.8013\n",
      "Epoch-115, Time-0.4616, Train-MSE-0.0085, Train-L2-0.0940, Test-L2-0.7723\n",
      "Epoch-116, Time-0.4555, Train-MSE-0.0108, Train-L2-0.1077, Test-L2-0.7934\n",
      "Epoch-117, Time-0.4495, Train-MSE-0.0083, Train-L2-0.0909, Test-L2-0.8267\n",
      "Epoch-118, Time-0.4932, Train-MSE-0.0089, Train-L2-0.1018, Test-L2-0.7846\n",
      "Epoch-119, Time-0.4358, Train-MSE-0.0091, Train-L2-0.0952, Test-L2-0.7625\n",
      "Epoch-120, Time-0.4952, Train-MSE-0.0085, Train-L2-0.0931, Test-L2-0.7951\n",
      "Epoch-121, Time-0.4530, Train-MSE-0.0073, Train-L2-0.0857, Test-L2-0.8042\n",
      "Epoch-122, Time-0.4527, Train-MSE-0.0071, Train-L2-0.0816, Test-L2-0.7906\n",
      "Epoch-123, Time-0.4478, Train-MSE-0.0067, Train-L2-0.0801, Test-L2-0.7880\n",
      "Epoch-124, Time-0.4487, Train-MSE-0.0067, Train-L2-0.0807, Test-L2-0.7914\n",
      "Epoch-125, Time-0.4502, Train-MSE-0.0067, Train-L2-0.0783, Test-L2-0.7804\n",
      "Epoch-126, Time-0.4775, Train-MSE-0.0067, Train-L2-0.0791, Test-L2-0.7911\n",
      "Epoch-127, Time-0.4848, Train-MSE-0.0065, Train-L2-0.0779, Test-L2-0.7919\n",
      "Epoch-128, Time-0.4442, Train-MSE-0.0067, Train-L2-0.0797, Test-L2-0.7788\n",
      "Epoch-129, Time-0.4958, Train-MSE-0.0055, Train-L2-0.0719, Test-L2-0.7856\n",
      "Epoch-130, Time-0.4526, Train-MSE-0.0056, Train-L2-0.0728, Test-L2-0.8031\n",
      "Epoch-131, Time-0.4348, Train-MSE-0.0057, Train-L2-0.0738, Test-L2-0.7835\n",
      "Epoch-132, Time-0.4570, Train-MSE-0.0052, Train-L2-0.0707, Test-L2-0.7742\n",
      "Epoch-133, Time-0.4499, Train-MSE-0.0051, Train-L2-0.0697, Test-L2-0.7861\n",
      "Epoch-134, Time-0.4454, Train-MSE-0.0057, Train-L2-0.0797, Test-L2-0.7874\n",
      "Epoch-135, Time-0.4621, Train-MSE-0.0053, Train-L2-0.0733, Test-L2-0.7676\n",
      "Epoch-136, Time-0.4960, Train-MSE-0.0056, Train-L2-0.0814, Test-L2-0.7940\n",
      "Epoch-137, Time-0.4894, Train-MSE-0.0068, Train-L2-0.0812, Test-L2-0.8320\n",
      "Epoch-138, Time-0.4416, Train-MSE-0.0058, Train-L2-0.0783, Test-L2-0.7842\n",
      "Epoch-139, Time-0.4502, Train-MSE-0.0052, Train-L2-0.0765, Test-L2-0.7663\n",
      "Epoch-140, Time-0.4403, Train-MSE-0.0045, Train-L2-0.0703, Test-L2-0.8244\n",
      "Epoch-141, Time-0.4508, Train-MSE-0.0081, Train-L2-0.1046, Test-L2-0.8179\n",
      "Epoch-142, Time-0.4985, Train-MSE-0.0041, Train-L2-0.0749, Test-L2-0.7670\n",
      "Epoch-143, Time-0.4774, Train-MSE-0.0063, Train-L2-0.0884, Test-L2-0.7770\n",
      "Epoch-144, Time-0.4394, Train-MSE-0.0037, Train-L2-0.0686, Test-L2-0.8151\n",
      "Epoch-145, Time-0.4479, Train-MSE-0.0058, Train-L2-0.0847, Test-L2-0.7956\n",
      "Epoch-146, Time-0.4577, Train-MSE-0.0044, Train-L2-0.0733, Test-L2-0.7542\n",
      "Epoch-147, Time-0.4522, Train-MSE-0.0084, Train-L2-0.0984, Test-L2-0.7643\n",
      "Epoch-148, Time-0.4587, Train-MSE-0.0066, Train-L2-0.0821, Test-L2-0.7826\n",
      "Epoch-149, Time-0.4405, Train-MSE-0.0044, Train-L2-0.0747, Test-L2-0.7836\n",
      "Epoch-150, Time-0.4588, Train-MSE-0.0031, Train-L2-0.0637, Test-L2-0.7842\n",
      "Epoch-151, Time-0.4666, Train-MSE-0.0031, Train-L2-0.0654, Test-L2-0.7938\n",
      "Epoch-152, Time-0.4576, Train-MSE-0.0025, Train-L2-0.0550, Test-L2-0.7984\n",
      "Epoch-153, Time-0.4742, Train-MSE-0.0028, Train-L2-0.0591, Test-L2-0.7796\n",
      "Epoch-154, Time-0.4380, Train-MSE-0.0026, Train-L2-0.0553, Test-L2-0.7777\n",
      "Epoch-155, Time-0.4748, Train-MSE-0.0026, Train-L2-0.0573, Test-L2-0.7955\n",
      "Epoch-156, Time-0.4801, Train-MSE-0.0024, Train-L2-0.0525, Test-L2-0.7934\n",
      "Epoch-157, Time-0.4397, Train-MSE-0.0022, Train-L2-0.0510, Test-L2-0.7953\n",
      "Epoch-158, Time-0.4411, Train-MSE-0.0024, Train-L2-0.0526, Test-L2-0.7901\n",
      "Epoch-159, Time-0.4429, Train-MSE-0.0022, Train-L2-0.0517, Test-L2-0.7808\n",
      "Epoch-160, Time-0.4405, Train-MSE-0.0028, Train-L2-0.0547, Test-L2-0.7869\n",
      "Epoch-161, Time-0.4408, Train-MSE-0.0020, Train-L2-0.0478, Test-L2-0.8049\n",
      "Epoch-162, Time-0.4341, Train-MSE-0.0020, Train-L2-0.0480, Test-L2-0.7924\n",
      "Epoch-163, Time-0.4337, Train-MSE-0.0019, Train-L2-0.0455, Test-L2-0.7891\n",
      "Epoch-164, Time-0.4326, Train-MSE-0.0017, Train-L2-0.0439, Test-L2-0.7931\n",
      "Epoch-165, Time-0.4602, Train-MSE-0.0016, Train-L2-0.0424, Test-L2-0.7850\n",
      "Epoch-166, Time-0.4467, Train-MSE-0.0017, Train-L2-0.0431, Test-L2-0.7866\n",
      "Epoch-167, Time-0.4334, Train-MSE-0.0014, Train-L2-0.0400, Test-L2-0.7984\n",
      "Epoch-168, Time-0.4445, Train-MSE-0.0014, Train-L2-0.0420, Test-L2-0.7924\n",
      "Epoch-169, Time-0.4317, Train-MSE-0.0016, Train-L2-0.0410, Test-L2-0.7913\n",
      "Epoch-170, Time-0.4720, Train-MSE-0.0015, Train-L2-0.0414, Test-L2-0.7986\n",
      "Epoch-171, Time-0.4293, Train-MSE-0.0014, Train-L2-0.0401, Test-L2-0.7861\n",
      "Epoch-172, Time-0.4474, Train-MSE-0.0018, Train-L2-0.0442, Test-L2-0.7941\n",
      "Epoch-173, Time-0.4331, Train-MSE-0.0013, Train-L2-0.0399, Test-L2-0.8032\n",
      "Epoch-174, Time-0.4820, Train-MSE-0.0013, Train-L2-0.0391, Test-L2-0.7912\n",
      "Epoch-175, Time-0.4741, Train-MSE-0.0014, Train-L2-0.0398, Test-L2-0.7910\n",
      "Epoch-176, Time-0.4285, Train-MSE-0.0015, Train-L2-0.0408, Test-L2-0.8027\n",
      "Epoch-177, Time-0.4334, Train-MSE-0.0014, Train-L2-0.0401, Test-L2-0.7881\n",
      "Epoch-178, Time-0.4326, Train-MSE-0.0015, Train-L2-0.0414, Test-L2-0.7925\n",
      "Epoch-179, Time-0.4293, Train-MSE-0.0014, Train-L2-0.0415, Test-L2-0.8009\n",
      "Epoch-180, Time-0.4282, Train-MSE-0.0011, Train-L2-0.0381, Test-L2-0.7873\n",
      "Epoch-181, Time-0.4262, Train-MSE-0.0012, Train-L2-0.0368, Test-L2-0.7927\n",
      "Epoch-182, Time-0.4222, Train-MSE-0.0011, Train-L2-0.0352, Test-L2-0.7984\n",
      "Epoch-183, Time-0.4259, Train-MSE-0.0010, Train-L2-0.0358, Test-L2-0.7931\n",
      "Epoch-184, Time-0.4219, Train-MSE-0.0010, Train-L2-0.0341, Test-L2-0.7972\n",
      "Epoch-185, Time-0.4208, Train-MSE-0.0008, Train-L2-0.0321, Test-L2-0.7912\n",
      "Epoch-186, Time-0.4294, Train-MSE-0.0010, Train-L2-0.0326, Test-L2-0.7950\n",
      "Epoch-187, Time-0.4286, Train-MSE-0.0010, Train-L2-0.0334, Test-L2-0.7964\n",
      "Epoch-188, Time-0.4294, Train-MSE-0.0009, Train-L2-0.0325, Test-L2-0.7897\n",
      "Epoch-189, Time-0.4282, Train-MSE-0.0009, Train-L2-0.0328, Test-L2-0.7972\n",
      "Epoch-190, Time-0.4238, Train-MSE-0.0008, Train-L2-0.0321, Test-L2-0.7888\n",
      "Epoch-191, Time-0.4273, Train-MSE-0.0010, Train-L2-0.0348, Test-L2-0.7985\n",
      "Epoch-192, Time-0.4284, Train-MSE-0.0010, Train-L2-0.0341, Test-L2-0.7962\n",
      "Epoch-193, Time-0.4281, Train-MSE-0.0009, Train-L2-0.0315, Test-L2-0.7884\n",
      "Epoch-194, Time-0.4635, Train-MSE-0.0008, Train-L2-0.0303, Test-L2-0.8029\n",
      "Epoch-195, Time-0.4599, Train-MSE-0.0009, Train-L2-0.0320, Test-L2-0.7966\n",
      "Epoch-196, Time-0.4432, Train-MSE-0.0009, Train-L2-0.0322, Test-L2-0.7988\n",
      "Epoch-197, Time-0.4352, Train-MSE-0.0008, Train-L2-0.0298, Test-L2-0.7998\n",
      "Epoch-198, Time-0.4464, Train-MSE-0.0009, Train-L2-0.0312, Test-L2-0.7915\n",
      "Epoch-199, Time-0.4513, Train-MSE-0.0008, Train-L2-0.0288, Test-L2-0.8015\n",
      "Epoch-200, Time-0.4289, Train-MSE-0.0007, Train-L2-0.0300, Test-L2-0.7962\n",
      "Epoch-201, Time-0.5030, Train-MSE-0.0007, Train-L2-0.0278, Test-L2-0.7945\n",
      "Epoch-202, Time-0.4499, Train-MSE-0.0006, Train-L2-0.0263, Test-L2-0.8055\n",
      "Epoch-203, Time-0.4524, Train-MSE-0.0007, Train-L2-0.0272, Test-L2-0.7945\n",
      "Epoch-204, Time-0.4489, Train-MSE-0.0006, Train-L2-0.0274, Test-L2-0.7981\n",
      "Epoch-205, Time-0.4491, Train-MSE-0.0005, Train-L2-0.0252, Test-L2-0.8041\n",
      "Epoch-206, Time-0.4548, Train-MSE-0.0006, Train-L2-0.0259, Test-L2-0.7890\n",
      "Epoch-207, Time-0.4910, Train-MSE-0.0006, Train-L2-0.0269, Test-L2-0.7959\n",
      "Epoch-208, Time-0.4656, Train-MSE-0.0006, Train-L2-0.0261, Test-L2-0.8027\n",
      "Epoch-209, Time-0.4384, Train-MSE-0.0006, Train-L2-0.0242, Test-L2-0.7941\n",
      "Epoch-210, Time-0.4653, Train-MSE-0.0006, Train-L2-0.0270, Test-L2-0.8039\n",
      "Epoch-211, Time-0.4325, Train-MSE-0.0007, Train-L2-0.0277, Test-L2-0.7986\n",
      "Epoch-212, Time-0.4342, Train-MSE-0.0006, Train-L2-0.0243, Test-L2-0.7870\n",
      "Epoch-213, Time-0.4317, Train-MSE-0.0008, Train-L2-0.0293, Test-L2-0.8002\n",
      "Epoch-214, Time-0.4277, Train-MSE-0.0006, Train-L2-0.0267, Test-L2-0.8001\n",
      "Epoch-215, Time-0.4338, Train-MSE-0.0006, Train-L2-0.0253, Test-L2-0.7944\n",
      "Epoch-216, Time-0.4407, Train-MSE-0.0007, Train-L2-0.0302, Test-L2-0.8041\n",
      "Epoch-217, Time-0.4358, Train-MSE-0.0006, Train-L2-0.0255, Test-L2-0.7978\n",
      "Epoch-218, Time-0.4363, Train-MSE-0.0006, Train-L2-0.0279, Test-L2-0.7999\n",
      "Epoch-219, Time-0.4385, Train-MSE-0.0005, Train-L2-0.0253, Test-L2-0.7953\n",
      "Epoch-220, Time-0.4720, Train-MSE-0.0006, Train-L2-0.0260, Test-L2-0.7988\n",
      "Epoch-221, Time-0.4605, Train-MSE-0.0005, Train-L2-0.0241, Test-L2-0.8040\n",
      "Epoch-222, Time-0.4334, Train-MSE-0.0005, Train-L2-0.0238, Test-L2-0.7983\n",
      "Epoch-223, Time-0.4386, Train-MSE-0.0005, Train-L2-0.0247, Test-L2-0.7981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-224, Time-0.4420, Train-MSE-0.0005, Train-L2-0.0244, Test-L2-0.7995\n",
      "Epoch-225, Time-0.4407, Train-MSE-0.0005, Train-L2-0.0255, Test-L2-0.8022\n",
      "Epoch-226, Time-0.4353, Train-MSE-0.0005, Train-L2-0.0254, Test-L2-0.8028\n",
      "Epoch-227, Time-0.4318, Train-MSE-0.0006, Train-L2-0.0270, Test-L2-0.7977\n",
      "Epoch-228, Time-0.4752, Train-MSE-0.0006, Train-L2-0.0269, Test-L2-0.7989\n",
      "Epoch-229, Time-0.4674, Train-MSE-0.0006, Train-L2-0.0262, Test-L2-0.8002\n",
      "Epoch-230, Time-0.4433, Train-MSE-0.0006, Train-L2-0.0265, Test-L2-0.8033\n",
      "Epoch-231, Time-0.4296, Train-MSE-0.0005, Train-L2-0.0242, Test-L2-0.8044\n",
      "Epoch-232, Time-0.4366, Train-MSE-0.0005, Train-L2-0.0247, Test-L2-0.7993\n",
      "Epoch-233, Time-0.4361, Train-MSE-0.0005, Train-L2-0.0253, Test-L2-0.8034\n",
      "Epoch-234, Time-0.4439, Train-MSE-0.0005, Train-L2-0.0246, Test-L2-0.8023\n",
      "Epoch-235, Time-0.4519, Train-MSE-0.0005, Train-L2-0.0237, Test-L2-0.8001\n",
      "Epoch-236, Time-0.4379, Train-MSE-0.0005, Train-L2-0.0231, Test-L2-0.8094\n",
      "Epoch-237, Time-0.4437, Train-MSE-0.0005, Train-L2-0.0246, Test-L2-0.8008\n",
      "Epoch-238, Time-0.4378, Train-MSE-0.0006, Train-L2-0.0254, Test-L2-0.8020\n",
      "Epoch-239, Time-0.4309, Train-MSE-0.0006, Train-L2-0.0253, Test-L2-0.8079\n",
      "Epoch-240, Time-0.4284, Train-MSE-0.0006, Train-L2-0.0253, Test-L2-0.8014\n",
      "Epoch-241, Time-0.4346, Train-MSE-0.0007, Train-L2-0.0278, Test-L2-0.8101\n",
      "Epoch-242, Time-0.4384, Train-MSE-0.0006, Train-L2-0.0267, Test-L2-0.8012\n",
      "Epoch-243, Time-0.4489, Train-MSE-0.0006, Train-L2-0.0285, Test-L2-0.8000\n",
      "Epoch-244, Time-0.4460, Train-MSE-0.0007, Train-L2-0.0286, Test-L2-0.8100\n",
      "Epoch-245, Time-0.4384, Train-MSE-0.0007, Train-L2-0.0305, Test-L2-0.8060\n",
      "Epoch-246, Time-0.4760, Train-MSE-0.0007, Train-L2-0.0302, Test-L2-0.8068\n",
      "Epoch-247, Time-0.4667, Train-MSE-0.0007, Train-L2-0.0316, Test-L2-0.8082\n",
      "Epoch-248, Time-0.4317, Train-MSE-0.0007, Train-L2-0.0321, Test-L2-0.8001\n",
      "Epoch-249, Time-0.4686, Train-MSE-0.0008, Train-L2-0.0334, Test-L2-0.8046\n"
     ]
    }
   ],
   "source": [
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer_hf.to(device)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader_hf:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x).reshape(x.shape[0], s, s)\n",
    "        out = y_normalizer_hf.decode(out)\n",
    "        y = y_normalizer_hf.decode(y)\n",
    "        \n",
    "        mse = F.mse_loss(out.view(x.shape[0], -1), y.view(x.shape[0], -1), reduction='mean')\n",
    "        loss = myloss(out.view(x.shape[0],-1), y.view(x.shape[0],-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_mse += mse.item()\n",
    "        train_l2 += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader_hf:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out = model(x).reshape(x.shape[0], s, s)\n",
    "            out = y_normalizer_hf.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(x.shape[0],-1), y.view(x.shape[0],-1)).item()\n",
    "\n",
    "    train_mse /= len(train_loader_hf)\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "    t2 = default_timer()\n",
    "    print('Epoch-{}, Time-{:0.4f}, Train-MSE-{:0.4f}, Train-L2-{:0.4f}, Test-L2-{:0.4f}'\n",
    "          .format(ep, t2-t1, train_mse, train_l2, test_l2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-XpQCiHwBRP"
   },
   "outputs": [],
   "source": [
    "# Save the HF-WNO model\n",
    "\n",
    "torch.save(model, 'model/HF_WNO_Darcy2D_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mei01tCV-zu5"
   },
   "outputs": [],
   "source": [
    "# Predict on HF data using HF-WNO\n",
    "pred_hf = [] \n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    for x, y in test_loader_mf:\n",
    "        x = x_normalizer_mf.decode(x)\n",
    "        x = x_normalizer_hf.encode(x)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        out = model(x[..., 0:1]).reshape(x.shape[0], s, s)\n",
    "        out = y_normalizer_hf.decode(out)\n",
    "        test_l2 = myloss(out.view(x.shape[0], -1), y.view(x.shape[0], -1)).item()\n",
    "        test_l2 /= x.shape[0]\n",
    "        print('Batch-{}, Test-L2-{:0.4f}'.format(index, test_l2))\n",
    "        \n",
    "        pred_hf.append(out.cpu())\n",
    "        index += 1\n",
    "\n",
    "pred_hf = torch.cat(( pred_hf ), dim=0 )\n",
    "\n",
    "print('Mean mse_hf-{}'.format(F.mse_loss(y_test_hf, pred_hf).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jshMxscLTUbX"
   },
   "outputs": [],
   "source": [
    "mse_pred_hf = F.mse_loss(pred_hf, y_test_hf).item()\n",
    "\n",
    "print('MSE-Predicted solution-{:0.4f}'.format(mse_pred_hf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs = plt.subplots(nrows=3, ncols=5, figsize=(16, 6), facecolor='w', edgecolor='k')\n",
    "fig2.subplots_adjust(hspace=0.35, wspace=0.2)\n",
    "\n",
    "fig2.suptitle(f'Predictions MFWNO AC2d Size', fontsize=16)\n",
    "index = 0 \n",
    "for sample in range(ntest):\n",
    "    if sample % 9 == 0:\n",
    "        im = axs[0, index].imshow(y_test_hf[sample, :, :], cmap='nipy_spectral',origin='lower')\n",
    "        plt.colorbar(im, ax=axs[0, index])\n",
    "        im = axs[1, index].imshow(pred_hf[sample, :, :], cmap='nipy_spectral',origin='lower')\n",
    "        plt.colorbar(im, ax=axs[1, index])\n",
    "        im = axs[2, index].imshow(torch.abs(y_test_hf[sample, :, :] - pred_hf[sample, :, :]),\n",
    "                                    cmap='jet',origin='lower')\n",
    "        plt.colorbar(im, ax=axs[2, index])\n",
    "        index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11133,
     "status": "ok",
     "timestamp": 1657836250311,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "hCMBZTp1FA4W",
    "outputId": "0e6552bd-1c0b-4483-c36a-eae4a188d48d"
   },
   "outputs": [],
   "source": [
    "fig4, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 4), facecolor='w', edgecolor='k')\n",
    "fig4.subplots_adjust(hspace=0.35, wspace=0.2)\n",
    "\n",
    "fig4.suptitle(f'Predictions Error', fontsize=16)\n",
    "index = 0 \n",
    "for sample in range(ntest):\n",
    "    if sample % 9 == 0:\n",
    "        im = axs[0, index].imshow(pred_hf[sample] - y_hf[sample], cmap='nipy_spectral',origin='lower')\n",
    "        plt.colorbar(im, ax=axs[0, index])\n",
    "        im = axs[1, index].imshow(output_mf[sample] - y_hf[sample], cmap='nipy_spectral',origin='lower')\n",
    "        plt.colorbar(im, ax=axs[1, index])\n",
    "        index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjRXr5V6rwjK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxvNKbP9uUwo"
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 7583,
     "status": "ok",
     "timestamp": 1658254523655,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "OrfneXu5S94x",
    "outputId": "9afbaf2b-e527-46a6-ae35-9cb7aafecf5e"
   },
   "outputs": [],
   "source": [
    "s = 1\n",
    "xmax = s\n",
    "ymax = s-8/51\n",
    "from matplotlib.patches import Rectangle\n",
    "plt.rcParams[\"font.family\"] = \"Serif\"\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "figure1 = plt.figure(constrained_layout=False, figsize = (14, 10))\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.3)\n",
    "index = 0\n",
    "value = 0\n",
    "\n",
    "plt.subplot(3,4, index+1)\n",
    "plt.imshow(output_mf[value,:,:], origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('MFSM');\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax]); plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.5,0),0.01,0.4, facecolor='white'))\n",
    "      \n",
    "      #####        \n",
    "plt.subplot(3,4, index+2)\n",
    "plt.imshow(pred_hf[value,:,:], origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('HFSM');\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax]); plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.5,0),0.01,0.4, facecolor='white'))\n",
    "        ###\n",
    "plt.subplot(3,4, index+3)\n",
    "# plt.imshow(pred.cpu().detach().numpy()[value,:,:], origin='lower', extent = [0, s, 0, s], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.imshow(input_mf[value,:,:,1], origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('LFM');\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax]); plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.5,0),0.01,0.41, facecolor='white'))\n",
    "        #####\n",
    "plt.subplot(3,4, index+4)\n",
    "# plt.imshow(pred.cpu().detach().numpy()[value,:,:], origin='lower', extent = [0, s, 0, s], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.imshow(real_mf[value,:,:], origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('Ground Truth');\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax]); plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.5,0),0.01,0.41, facecolor='white'))\n",
    "        \n",
    "        ###\n",
    "plt.subplot(3,4, index+5)\n",
    "plt.imshow(np.abs(output_mf[value,:,:]-real_mf[value,:,:]), \\\n",
    "   origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('Error');\n",
    "plt.colorbar(orientation=\"horizontal\", fraction=0.04, pad=0.2)\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax])\n",
    "plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.49,0),0.01,0.41, facecolor='white'))\n",
    "\n",
    "        ###\n",
    "plt.subplot(3,4, index+6)\n",
    "plt.imshow(np.abs(pred_hf[value,:,:]-real_mf[value,:,:]), \\\n",
    "   origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('Error');\n",
    "plt.colorbar(orientation=\"horizontal\", fraction=0.04, pad=0.2)\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax])\n",
    "plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.49,0),0.01,0.41, facecolor='white'))\n",
    "\n",
    "\n",
    "        ###\n",
    "plt.subplot(3,4, index+7)\n",
    "plt.imshow(np.abs(input_mf[value,:,:,1]-real_mf[value,:,:]), \\\n",
    "   origin='lower', extent = [0, 1, 0, 1], interpolation='Gaussian', cmap='nipy_spectral')\n",
    "plt.title('Error');\n",
    "plt.colorbar(orientation=\"horizontal\", fraction=0.04, pad=0.2)\n",
    "        \n",
    "xf = np.array([0., xmax/2]); yf = xf*(ymax/(xmax/2)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([xmax/2, xmax]); yf = (xf-xmax)*(ymax/((xmax/2)-xmax)); plt.fill_between(xf, yf, ymax, color = [1, 1, 1])\n",
    "xf = np.array([0, xmax])\n",
    "plt.fill_between(xf, ymax, s, color = [1, 1, 1])        \n",
    "plt.gca().add_patch(Rectangle((0.49,0),0.01,0.41, facecolor='white'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPUFVlzIY3s+OcoPYM/dcWf",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Multifid_WNO_Darcy_notch_size_30.ipynb",
   "provenance": [
    {
     "file_id": "1OPIb4ygLODwEKnHrahQ__DiDVyCq6543",
     "timestamp": 1656351883644
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
