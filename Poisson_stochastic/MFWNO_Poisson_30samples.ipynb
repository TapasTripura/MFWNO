{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It trains MFWNO on the MF 1D Poisson's data (time-independent problem).\n",
    "### HF data size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1658147804702,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "ce-T3Tu5zyE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer\n",
    "from pytorch_wavelets import DWT1D, IDWT1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1658147804703,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "8JbWtxniSerT"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ds09MG-EOKw"
   },
   "source": [
    "# WNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1658147806420,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "p6rDXElhEQIq"
   },
   "outputs": [],
   "source": [
    "class WaveConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, level, size, wavelet):\n",
    "        super(WaveConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Wavelet layer. It does Wavelet Transform, linear transform, and\n",
    "        Inverse Wavelet Transform.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.level = level\n",
    "        self.wavelet = wavelet \n",
    "        self.dwt_ = DWT1D(wave=self.wavelet, J=self.level, mode='zero')\n",
    "        dummy_data = torch.randn( 1,1,size ) \n",
    "        mode_data, _ = self.dwt_(dummy_data)\n",
    "        self.modes1 = mode_data.shape[-1]\n",
    "        \n",
    "        # Parameter initilization\n",
    "        self.scale = (1 / (in_channels*out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1))\n",
    "\n",
    "    # Convolution\n",
    "    def mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input parameters: \n",
    "        -----------------\n",
    "        x : tensor, shape-[Batch * Channel * x]\n",
    "        Output parameters: \n",
    "        ------------------\n",
    "        x : tensor, shape-[Batch * Channel * x]\n",
    "        \"\"\"\n",
    "        # Compute single tree Discrete Wavelet coefficients using some wavelet     \n",
    "        dwt = DWT1D(wave=self.wavelet, J=self.level, mode='zero').to(x.device)\n",
    "        x_ft, x_coeff = dwt(x)\n",
    "        \n",
    "        # Multiply the final low pass wavelet coefficients\n",
    "        out_ft = self.mul1d(x_ft, self.weights1)\n",
    "        # Multiply the final high pass wavelet coefficients\n",
    "        x_coeff[-1] = self.mul1d(x_coeff[-1].clone(), self.weights2)\n",
    "        \n",
    "        # Reconstruct the signal\n",
    "        idwt = IDWT1D(wave=self.wavelet, mode='zero').to(x.device)\n",
    "        x = idwt((out_ft, x_coeff)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1658147806422,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "5ZqLQP5bEQ3s"
   },
   "outputs": [],
   "source": [
    "class WNO1d(nn.Module):\n",
    "    def __init__(self, width, level, size, wavelet, in_channel, grid_range):\n",
    "        super(WNO1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The WNO network. It contains 4 layers of the Wavelet integral layer.\n",
    "        1. Lift the input using v(x) = self.fc0 .\n",
    "        2. 4 layers of the integral operators v(+1) = g(K(.) + W)(v).\n",
    "            W is defined by self.w_; K is defined by self.conv_.\n",
    "        3. Project the output of last layer using self.fc1 and self.fc2.\n",
    "        \n",
    "        input: the solution of the initial condition and location (a(x), x)\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution of a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.level = level\n",
    "        self.width = width\n",
    "        self.size = size\n",
    "        self.wavelet = wavelet\n",
    "        self.in_channel = in_channel\n",
    "        self.grid_range = grid_range \n",
    "        self.padding = 2\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.in_channel, self.width) # input channel is 2: (a(x), x)\n",
    "\n",
    "        self.conv0 = WaveConv1d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv1 = WaveConv1d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv2 = WaveConv1d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv3 = WaveConv1d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        if self.padding != 0:\n",
    "            x = F.pad(x, [0,self.padding]) # do padding, if required\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "        \n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        if self.padding != 0:\n",
    "            x = x[..., :-self.padding] # remove padding, when required\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        # The grid of the solution\n",
    "        batchsize, size_x = shape[0], shape[1]\n",
    "        gridx = torch.tensor(np.linspace(0, self.grid_range, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
    "        return gridx.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1658147806422,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "nS7lMpnW6xWI"
   },
   "outputs": [],
   "source": [
    "class WNO1d_linear(nn.Module):\n",
    "    def __init__(self, width, level, size, wavelet, in_channel, grid_range):\n",
    "        super(WNO1d_linear, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The WNO network. It contains 4 layers of the Wavelet integral layer.\n",
    "        1. Lift the input using v(x) = self.fc0 .\n",
    "        2. 4 layers of the integral operators v(+1) = g(K(.) + W)(v).\n",
    "            W is defined by self.w_; K is defined by self.conv_.\n",
    "        3. Project the output of last layer using self.fc1 and self.fc2.\n",
    "        \n",
    "        input: the solution of the initial condition and location (a(x), x)\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution of a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.level = level\n",
    "        self.width = width\n",
    "        self.size = size\n",
    "        self.wavelet = wavelet\n",
    "        self.in_channel = in_channel\n",
    "        self.grid_range = grid_range \n",
    "        self.padding = 2\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.in_channel, self.width) # input channel is 2: (a(x), x)\n",
    "\n",
    "        self.conv0 = WaveConv1d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.conv1 = WaveConv1d(self.width, self.width, self.level, self.size, self.wavelet)\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.pad(x, [0,self.padding]) # do padding, if required\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding] # remove padding, when required\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        # The grid of the solution\n",
    "        batchsize, size_x = shape[0], shape[1]\n",
    "        gridx = torch.tensor(np.linspace(0, self.grid_range, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
    "        return gridx.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1658147806424,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "6JlOIBThdPG4"
   },
   "outputs": [],
   "source": [
    "class MFWNO(nn.Module):\n",
    "  def __init__(self, width, level, size, wavelet, in_channel, grid_range):\n",
    "    super(MFWNO, self).__init__()\n",
    "    \n",
    "    self.width = width\n",
    "    self.level = level \n",
    "    self.size = size\n",
    "    self.wavelet = wavelet \n",
    "    self.in_channel = in_channel \n",
    "    self.grid_range = grid_range\n",
    "    \n",
    "    self.conv1 = WNO1d_linear(self.width, self.level, self.size, self.wavelet, self.in_channel, self.grid_range)\n",
    "    self.conv2 = WNO1d(self.width, self.level, self.size, self.wavelet, self.in_channel, self.grid_range)\n",
    "    self.fc0 = nn.Linear(1,12)\n",
    "    self.fc1 = nn.Linear(12,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x) + self.conv2(x)\n",
    "    x = self.fc0(x)\n",
    "    x = F.gelu(x)\n",
    "    x = self.fc1(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKe4bEBlRd6K"
   },
   "source": [
    "# Multifidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1658147826073,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "EZbcADk9-msh"
   },
   "outputs": [],
   "source": [
    "ntrain = 30\n",
    "ntest = 40\n",
    "n_total = ntrain + ntest\n",
    "last_m = 400\n",
    "s = 100\n",
    "\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "w_decay = 1e-4\n",
    "epochs = 500\n",
    "step_size = 50   # weight-decay step size\n",
    "gamma = 0.5      # weight-decay rate\n",
    "\n",
    "wavelet = 'db6'  # wavelet basis function\n",
    "level = 3        # lavel of wavelet decomposition\n",
    "width = 64       # uplifting dimension\n",
    "layers = 4       # no of wavelet layers\n",
    "\n",
    "h = 100           # total grid size divided by the subsampling rate\n",
    "grid_range = 1\n",
    "in_channel = 3   # (a(x), x) for this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1658147826758,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "0Mg9uu2UYYQT"
   },
   "outputs": [],
   "source": [
    "PATH = 'data/possion_10pt_100pt__lscale_01.npz'\n",
    "data = np.load(PATH)\n",
    "\n",
    "x_data_h = data['f_stoch'][last_m-n_total:last_m]\n",
    "y_data_l = data['y_low_100'][last_m-n_total:last_m]\n",
    "y_data_h = data['yhi'][last_m-n_total:last_m]\n",
    "x_coords = data['xhi'].reshape((s,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data['f_stoch'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1658147826759,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "qW4O4wil7eGh"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "x_mf = np.stack((x_data_h, y_data_l), axis=-1)\n",
    "y_mf = y_data_h - y_data_l\n",
    "\n",
    "x_train_mf, y_train_mf = x_mf[:ntrain, ...], y_mf[:ntrain, ...]\n",
    "x_test_mf, y_test_mf = x_mf[-ntest:, ...], y_mf[-ntest:, ...]\n",
    "\n",
    "x_train_mf = torch.tensor( x_train_mf, dtype=torch.float )\n",
    "y_train_mf = torch.tensor( y_train_mf, dtype=torch.float ) \n",
    "x_test_mf = torch.tensor( x_test_mf, dtype=torch.float ) \n",
    "y_test_mf = torch.tensor( y_test_mf, dtype=torch.float ) \n",
    "\n",
    "train_loader_mf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_mf, y_train_mf),\n",
    "                                              batch_size=batch_size, shuffle=True)\n",
    "test_loader_mf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_mf, y_test_mf),\n",
    "                                             batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1658147826761,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "NcMsmj0TH5CV"
   },
   "outputs": [],
   "source": [
    "print(x_mf.shape, y_mf.shape, x_train_mf.shape, y_train_mf.shape, x_test_mf.shape, y_test_mf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1658147834295,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "3eomwdLl-t-f",
    "outputId": "c6cb9290-3eda-4cac-aa0b-257a5cca5571"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_mf = WNO1d(width=width, level=4, size=h, wavelet=wavelet,\n",
    "              in_channel=3, grid_range=grid_range).to(device)\n",
    "print(count_params(model_mf))\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mf.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121427,
     "status": "ok",
     "timestamp": 1658147955706,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "Q5KPrqWE6O1x",
    "outputId": "556fdec7-0e66-40fd-c8b1-bd7de3da022b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-92, Time-0.2585, Train-MSE-0.0000, Train-L2-0.1034, Test-L2-0.7560, Test-MSE-0.0009\n",
      "Epoch-93, Time-0.2769, Train-MSE-0.0000, Train-L2-0.0723, Test-L2-0.7478, Test-MSE-0.0009\n",
      "Epoch-94, Time-0.2671, Train-MSE-0.0000, Train-L2-0.0789, Test-L2-0.7605, Test-MSE-0.0009\n",
      "Epoch-95, Time-0.2685, Train-MSE-0.0000, Train-L2-0.0763, Test-L2-0.7494, Test-MSE-0.0009\n",
      "Epoch-96, Time-0.2752, Train-MSE-0.0000, Train-L2-0.0873, Test-L2-0.7534, Test-MSE-0.0009\n",
      "Epoch-97, Time-0.2545, Train-MSE-0.0000, Train-L2-0.0903, Test-L2-0.7501, Test-MSE-0.0009\n",
      "Epoch-98, Time-0.2632, Train-MSE-0.0000, Train-L2-0.0796, Test-L2-0.7607, Test-MSE-0.0009\n",
      "Epoch-99, Time-0.2504, Train-MSE-0.0000, Train-L2-0.0967, Test-L2-0.7624, Test-MSE-0.0009\n",
      "Epoch-100, Time-0.2436, Train-MSE-0.0000, Train-L2-0.0822, Test-L2-0.7488, Test-MSE-0.0009\n",
      "Epoch-101, Time-0.2346, Train-MSE-0.0000, Train-L2-0.0816, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-102, Time-0.2389, Train-MSE-0.0000, Train-L2-0.0740, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-103, Time-0.2350, Train-MSE-0.0000, Train-L2-0.0606, Test-L2-0.7490, Test-MSE-0.0009\n",
      "Epoch-104, Time-0.2384, Train-MSE-0.0000, Train-L2-0.0606, Test-L2-0.7574, Test-MSE-0.0009\n",
      "Epoch-105, Time-0.2319, Train-MSE-0.0000, Train-L2-0.0605, Test-L2-0.7523, Test-MSE-0.0009\n",
      "Epoch-106, Time-0.2368, Train-MSE-0.0000, Train-L2-0.0564, Test-L2-0.7538, Test-MSE-0.0009\n",
      "Epoch-107, Time-0.2356, Train-MSE-0.0000, Train-L2-0.0548, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-108, Time-0.2405, Train-MSE-0.0000, Train-L2-0.0497, Test-L2-0.7582, Test-MSE-0.0009\n",
      "Epoch-109, Time-0.2330, Train-MSE-0.0000, Train-L2-0.0420, Test-L2-0.7550, Test-MSE-0.0009\n",
      "Epoch-110, Time-0.2382, Train-MSE-0.0000, Train-L2-0.0456, Test-L2-0.7533, Test-MSE-0.0009\n",
      "Epoch-111, Time-0.2355, Train-MSE-0.0000, Train-L2-0.0476, Test-L2-0.7528, Test-MSE-0.0009\n",
      "Epoch-112, Time-0.2778, Train-MSE-0.0000, Train-L2-0.0390, Test-L2-0.7584, Test-MSE-0.0009\n",
      "Epoch-113, Time-0.2696, Train-MSE-0.0000, Train-L2-0.0458, Test-L2-0.7510, Test-MSE-0.0009\n",
      "Epoch-114, Time-0.2529, Train-MSE-0.0000, Train-L2-0.0420, Test-L2-0.7599, Test-MSE-0.0009\n",
      "Epoch-115, Time-0.2672, Train-MSE-0.0000, Train-L2-0.0464, Test-L2-0.7612, Test-MSE-0.0009\n",
      "Epoch-116, Time-0.2657, Train-MSE-0.0000, Train-L2-0.0584, Test-L2-0.7589, Test-MSE-0.0009\n",
      "Epoch-117, Time-0.2889, Train-MSE-0.0000, Train-L2-0.0623, Test-L2-0.7525, Test-MSE-0.0009\n",
      "Epoch-118, Time-0.2797, Train-MSE-0.0000, Train-L2-0.0605, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-119, Time-0.2573, Train-MSE-0.0000, Train-L2-0.0586, Test-L2-0.7594, Test-MSE-0.0009\n",
      "Epoch-120, Time-0.2758, Train-MSE-0.0000, Train-L2-0.0597, Test-L2-0.7528, Test-MSE-0.0009\n",
      "Epoch-121, Time-0.2723, Train-MSE-0.0000, Train-L2-0.0506, Test-L2-0.7513, Test-MSE-0.0009\n",
      "Epoch-122, Time-0.2950, Train-MSE-0.0000, Train-L2-0.0521, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-123, Time-0.2934, Train-MSE-0.0000, Train-L2-0.0454, Test-L2-0.7553, Test-MSE-0.0009\n",
      "Epoch-124, Time-0.2839, Train-MSE-0.0000, Train-L2-0.0513, Test-L2-0.7469, Test-MSE-0.0009\n",
      "Epoch-125, Time-0.2871, Train-MSE-0.0000, Train-L2-0.0530, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-126, Time-0.2892, Train-MSE-0.0000, Train-L2-0.0505, Test-L2-0.7534, Test-MSE-0.0009\n",
      "Epoch-127, Time-0.2791, Train-MSE-0.0000, Train-L2-0.0542, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-128, Time-0.2882, Train-MSE-0.0000, Train-L2-0.0451, Test-L2-0.7542, Test-MSE-0.0009\n",
      "Epoch-129, Time-0.2877, Train-MSE-0.0000, Train-L2-0.0502, Test-L2-0.7547, Test-MSE-0.0009\n",
      "Epoch-130, Time-0.2838, Train-MSE-0.0000, Train-L2-0.0437, Test-L2-0.7574, Test-MSE-0.0009\n",
      "Epoch-131, Time-0.2674, Train-MSE-0.0000, Train-L2-0.0427, Test-L2-0.7585, Test-MSE-0.0009\n",
      "Epoch-132, Time-0.2762, Train-MSE-0.0000, Train-L2-0.0461, Test-L2-0.7561, Test-MSE-0.0009\n",
      "Epoch-133, Time-0.2896, Train-MSE-0.0000, Train-L2-0.0417, Test-L2-0.7546, Test-MSE-0.0009\n",
      "Epoch-134, Time-0.2826, Train-MSE-0.0000, Train-L2-0.0472, Test-L2-0.7563, Test-MSE-0.0009\n",
      "Epoch-135, Time-0.2836, Train-MSE-0.0000, Train-L2-0.0423, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-136, Time-0.2713, Train-MSE-0.0000, Train-L2-0.0371, Test-L2-0.7577, Test-MSE-0.0009\n",
      "Epoch-137, Time-0.2648, Train-MSE-0.0000, Train-L2-0.0398, Test-L2-0.7529, Test-MSE-0.0009\n",
      "Epoch-138, Time-0.2742, Train-MSE-0.0000, Train-L2-0.0382, Test-L2-0.7589, Test-MSE-0.0009\n",
      "Epoch-139, Time-0.2909, Train-MSE-0.0000, Train-L2-0.0451, Test-L2-0.7551, Test-MSE-0.0009\n",
      "Epoch-140, Time-0.2718, Train-MSE-0.0000, Train-L2-0.0476, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-141, Time-0.2774, Train-MSE-0.0000, Train-L2-0.0458, Test-L2-0.7532, Test-MSE-0.0009\n",
      "Epoch-142, Time-0.2760, Train-MSE-0.0000, Train-L2-0.0514, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-143, Time-0.2919, Train-MSE-0.0000, Train-L2-0.0501, Test-L2-0.7617, Test-MSE-0.0009\n",
      "Epoch-144, Time-0.2591, Train-MSE-0.0000, Train-L2-0.0447, Test-L2-0.7536, Test-MSE-0.0009\n",
      "Epoch-145, Time-0.2685, Train-MSE-0.0000, Train-L2-0.0400, Test-L2-0.7528, Test-MSE-0.0009\n",
      "Epoch-146, Time-0.2782, Train-MSE-0.0000, Train-L2-0.0430, Test-L2-0.7564, Test-MSE-0.0009\n",
      "Epoch-147, Time-0.2515, Train-MSE-0.0000, Train-L2-0.0391, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-148, Time-0.2705, Train-MSE-0.0000, Train-L2-0.0353, Test-L2-0.7556, Test-MSE-0.0009\n",
      "Epoch-149, Time-0.2968, Train-MSE-0.0000, Train-L2-0.0399, Test-L2-0.7596, Test-MSE-0.0009\n",
      "Epoch-150, Time-0.2681, Train-MSE-0.0000, Train-L2-0.0396, Test-L2-0.7535, Test-MSE-0.0009\n",
      "Epoch-151, Time-0.2941, Train-MSE-0.0000, Train-L2-0.0306, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-152, Time-0.2885, Train-MSE-0.0000, Train-L2-0.0278, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-153, Time-0.2744, Train-MSE-0.0000, Train-L2-0.0256, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-154, Time-0.2601, Train-MSE-0.0000, Train-L2-0.0244, Test-L2-0.7548, Test-MSE-0.0009\n",
      "Epoch-155, Time-0.2544, Train-MSE-0.0000, Train-L2-0.0239, Test-L2-0.7575, Test-MSE-0.0009\n",
      "Epoch-156, Time-0.2441, Train-MSE-0.0000, Train-L2-0.0231, Test-L2-0.7541, Test-MSE-0.0009\n",
      "Epoch-157, Time-0.2522, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.7577, Test-MSE-0.0009\n",
      "Epoch-158, Time-0.2578, Train-MSE-0.0000, Train-L2-0.0231, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-159, Time-0.2465, Train-MSE-0.0000, Train-L2-0.0258, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-160, Time-0.2401, Train-MSE-0.0000, Train-L2-0.0230, Test-L2-0.7573, Test-MSE-0.0009\n",
      "Epoch-161, Time-0.2366, Train-MSE-0.0000, Train-L2-0.0230, Test-L2-0.7544, Test-MSE-0.0009\n",
      "Epoch-162, Time-0.2544, Train-MSE-0.0000, Train-L2-0.0211, Test-L2-0.7552, Test-MSE-0.0009\n",
      "Epoch-163, Time-0.2721, Train-MSE-0.0000, Train-L2-0.0228, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-164, Time-0.2400, Train-MSE-0.0000, Train-L2-0.0242, Test-L2-0.7551, Test-MSE-0.0009\n",
      "Epoch-165, Time-0.2652, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.7587, Test-MSE-0.0009\n",
      "Epoch-166, Time-0.2686, Train-MSE-0.0000, Train-L2-0.0216, Test-L2-0.7551, Test-MSE-0.0009\n",
      "Epoch-167, Time-0.2819, Train-MSE-0.0000, Train-L2-0.0234, Test-L2-0.7548, Test-MSE-0.0009\n",
      "Epoch-168, Time-0.2870, Train-MSE-0.0000, Train-L2-0.0232, Test-L2-0.7560, Test-MSE-0.0009\n",
      "Epoch-169, Time-0.2794, Train-MSE-0.0000, Train-L2-0.0268, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-170, Time-0.2823, Train-MSE-0.0000, Train-L2-0.0247, Test-L2-0.7560, Test-MSE-0.0009\n",
      "Epoch-171, Time-0.2992, Train-MSE-0.0000, Train-L2-0.0305, Test-L2-0.7562, Test-MSE-0.0009\n",
      "Epoch-172, Time-0.2721, Train-MSE-0.0000, Train-L2-0.0308, Test-L2-0.7534, Test-MSE-0.0009\n",
      "Epoch-173, Time-0.2861, Train-MSE-0.0000, Train-L2-0.0257, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-174, Time-0.2958, Train-MSE-0.0000, Train-L2-0.0231, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-175, Time-0.2993, Train-MSE-0.0000, Train-L2-0.0286, Test-L2-0.7547, Test-MSE-0.0009\n",
      "Epoch-176, Time-0.2945, Train-MSE-0.0000, Train-L2-0.0254, Test-L2-0.7559, Test-MSE-0.0009\n",
      "Epoch-177, Time-0.2859, Train-MSE-0.0000, Train-L2-0.0302, Test-L2-0.7580, Test-MSE-0.0009\n",
      "Epoch-178, Time-0.2916, Train-MSE-0.0000, Train-L2-0.0251, Test-L2-0.7552, Test-MSE-0.0009\n",
      "Epoch-179, Time-0.2909, Train-MSE-0.0000, Train-L2-0.0261, Test-L2-0.7564, Test-MSE-0.0009\n",
      "Epoch-180, Time-0.2947, Train-MSE-0.0000, Train-L2-0.0236, Test-L2-0.7580, Test-MSE-0.0009\n",
      "Epoch-181, Time-0.3047, Train-MSE-0.0000, Train-L2-0.0233, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-182, Time-0.2801, Train-MSE-0.0000, Train-L2-0.0274, Test-L2-0.7566, Test-MSE-0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-183, Time-0.2696, Train-MSE-0.0000, Train-L2-0.0270, Test-L2-0.7581, Test-MSE-0.0009\n",
      "Epoch-184, Time-0.2653, Train-MSE-0.0000, Train-L2-0.0242, Test-L2-0.7545, Test-MSE-0.0009\n",
      "Epoch-185, Time-0.2856, Train-MSE-0.0000, Train-L2-0.0245, Test-L2-0.7542, Test-MSE-0.0009\n",
      "Epoch-186, Time-0.2888, Train-MSE-0.0000, Train-L2-0.0225, Test-L2-0.7577, Test-MSE-0.0009\n",
      "Epoch-187, Time-0.2757, Train-MSE-0.0000, Train-L2-0.0227, Test-L2-0.7555, Test-MSE-0.0009\n",
      "Epoch-188, Time-0.2933, Train-MSE-0.0000, Train-L2-0.0213, Test-L2-0.7553, Test-MSE-0.0009\n",
      "Epoch-189, Time-0.2947, Train-MSE-0.0000, Train-L2-0.0251, Test-L2-0.7559, Test-MSE-0.0009\n",
      "Epoch-190, Time-0.2995, Train-MSE-0.0000, Train-L2-0.0245, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-191, Time-0.3042, Train-MSE-0.0000, Train-L2-0.0227, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-192, Time-0.3092, Train-MSE-0.0000, Train-L2-0.0240, Test-L2-0.7555, Test-MSE-0.0009\n",
      "Epoch-193, Time-0.3052, Train-MSE-0.0000, Train-L2-0.0224, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-194, Time-0.2659, Train-MSE-0.0000, Train-L2-0.0227, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-195, Time-0.2557, Train-MSE-0.0000, Train-L2-0.0220, Test-L2-0.7545, Test-MSE-0.0009\n",
      "Epoch-196, Time-0.2612, Train-MSE-0.0000, Train-L2-0.0220, Test-L2-0.7593, Test-MSE-0.0009\n",
      "Epoch-197, Time-0.2550, Train-MSE-0.0000, Train-L2-0.0223, Test-L2-0.7541, Test-MSE-0.0009\n",
      "Epoch-198, Time-0.2579, Train-MSE-0.0000, Train-L2-0.0229, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-199, Time-0.2586, Train-MSE-0.0000, Train-L2-0.0259, Test-L2-0.7581, Test-MSE-0.0009\n",
      "Epoch-200, Time-0.2553, Train-MSE-0.0000, Train-L2-0.0253, Test-L2-0.7554, Test-MSE-0.0009\n",
      "Epoch-201, Time-0.2713, Train-MSE-0.0000, Train-L2-0.0241, Test-L2-0.7576, Test-MSE-0.0009\n",
      "Epoch-202, Time-0.2673, Train-MSE-0.0000, Train-L2-0.0227, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-203, Time-0.2714, Train-MSE-0.0000, Train-L2-0.0190, Test-L2-0.7553, Test-MSE-0.0009\n",
      "Epoch-204, Time-0.2678, Train-MSE-0.0000, Train-L2-0.0172, Test-L2-0.7562, Test-MSE-0.0009\n",
      "Epoch-205, Time-0.2775, Train-MSE-0.0000, Train-L2-0.0159, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-206, Time-0.3114, Train-MSE-0.0000, Train-L2-0.0155, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-207, Time-0.3087, Train-MSE-0.0000, Train-L2-0.0157, Test-L2-0.7563, Test-MSE-0.0009\n",
      "Epoch-208, Time-0.3006, Train-MSE-0.0000, Train-L2-0.0155, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-209, Time-0.2955, Train-MSE-0.0000, Train-L2-0.0155, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-210, Time-0.2825, Train-MSE-0.0000, Train-L2-0.0160, Test-L2-0.7559, Test-MSE-0.0009\n",
      "Epoch-211, Time-0.2807, Train-MSE-0.0000, Train-L2-0.0146, Test-L2-0.7564, Test-MSE-0.0009\n",
      "Epoch-212, Time-0.2999, Train-MSE-0.0000, Train-L2-0.0148, Test-L2-0.7558, Test-MSE-0.0009\n",
      "Epoch-213, Time-0.3197, Train-MSE-0.0000, Train-L2-0.0145, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-214, Time-0.2770, Train-MSE-0.0000, Train-L2-0.0153, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-215, Time-0.2939, Train-MSE-0.0000, Train-L2-0.0164, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-216, Time-0.3038, Train-MSE-0.0000, Train-L2-0.0146, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-217, Time-0.3006, Train-MSE-0.0000, Train-L2-0.0139, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-218, Time-0.3068, Train-MSE-0.0000, Train-L2-0.0148, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-219, Time-0.3058, Train-MSE-0.0000, Train-L2-0.0147, Test-L2-0.7556, Test-MSE-0.0009\n",
      "Epoch-220, Time-0.2768, Train-MSE-0.0000, Train-L2-0.0150, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-221, Time-0.2997, Train-MSE-0.0000, Train-L2-0.0152, Test-L2-0.7564, Test-MSE-0.0009\n",
      "Epoch-222, Time-0.3052, Train-MSE-0.0000, Train-L2-0.0155, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-223, Time-0.2889, Train-MSE-0.0000, Train-L2-0.0151, Test-L2-0.7560, Test-MSE-0.0009\n",
      "Epoch-224, Time-0.3031, Train-MSE-0.0000, Train-L2-0.0147, Test-L2-0.7559, Test-MSE-0.0009\n",
      "Epoch-225, Time-0.3059, Train-MSE-0.0000, Train-L2-0.0144, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-226, Time-0.3056, Train-MSE-0.0000, Train-L2-0.0140, Test-L2-0.7556, Test-MSE-0.0009\n",
      "Epoch-227, Time-0.2745, Train-MSE-0.0000, Train-L2-0.0144, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-228, Time-0.2937, Train-MSE-0.0000, Train-L2-0.0152, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-229, Time-0.3033, Train-MSE-0.0000, Train-L2-0.0153, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-230, Time-0.2844, Train-MSE-0.0000, Train-L2-0.0159, Test-L2-0.7555, Test-MSE-0.0009\n",
      "Epoch-231, Time-0.2975, Train-MSE-0.0000, Train-L2-0.0165, Test-L2-0.7553, Test-MSE-0.0009\n",
      "Epoch-232, Time-0.2893, Train-MSE-0.0000, Train-L2-0.0174, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-233, Time-0.2953, Train-MSE-0.0000, Train-L2-0.0188, Test-L2-0.7580, Test-MSE-0.0009\n",
      "Epoch-234, Time-0.2539, Train-MSE-0.0000, Train-L2-0.0167, Test-L2-0.7561, Test-MSE-0.0009\n",
      "Epoch-235, Time-0.2534, Train-MSE-0.0000, Train-L2-0.0160, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-236, Time-0.2641, Train-MSE-0.0000, Train-L2-0.0153, Test-L2-0.7559, Test-MSE-0.0009\n",
      "Epoch-237, Time-0.2698, Train-MSE-0.0000, Train-L2-0.0146, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-238, Time-0.2692, Train-MSE-0.0000, Train-L2-0.0149, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-239, Time-0.2692, Train-MSE-0.0000, Train-L2-0.0141, Test-L2-0.7561, Test-MSE-0.0009\n",
      "Epoch-240, Time-0.2658, Train-MSE-0.0000, Train-L2-0.0146, Test-L2-0.7563, Test-MSE-0.0009\n",
      "Epoch-241, Time-0.2692, Train-MSE-0.0000, Train-L2-0.0150, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-242, Time-0.2700, Train-MSE-0.0000, Train-L2-0.0145, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-243, Time-0.2656, Train-MSE-0.0000, Train-L2-0.0147, Test-L2-0.7556, Test-MSE-0.0009\n",
      "Epoch-244, Time-0.2686, Train-MSE-0.0000, Train-L2-0.0157, Test-L2-0.7577, Test-MSE-0.0009\n",
      "Epoch-245, Time-0.2655, Train-MSE-0.0000, Train-L2-0.0150, Test-L2-0.7575, Test-MSE-0.0009\n",
      "Epoch-246, Time-0.2641, Train-MSE-0.0000, Train-L2-0.0141, Test-L2-0.7560, Test-MSE-0.0009\n",
      "Epoch-247, Time-0.2711, Train-MSE-0.0000, Train-L2-0.0146, Test-L2-0.7563, Test-MSE-0.0009\n",
      "Epoch-248, Time-0.2728, Train-MSE-0.0000, Train-L2-0.0152, Test-L2-0.7558, Test-MSE-0.0009\n",
      "Epoch-249, Time-0.2710, Train-MSE-0.0000, Train-L2-0.0152, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-250, Time-0.2780, Train-MSE-0.0000, Train-L2-0.0142, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-251, Time-0.3014, Train-MSE-0.0000, Train-L2-0.0124, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-252, Time-0.3086, Train-MSE-0.0000, Train-L2-0.0122, Test-L2-0.7562, Test-MSE-0.0009\n",
      "Epoch-253, Time-0.3086, Train-MSE-0.0000, Train-L2-0.0116, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-254, Time-0.3085, Train-MSE-0.0000, Train-L2-0.0114, Test-L2-0.7563, Test-MSE-0.0009\n",
      "Epoch-255, Time-0.2987, Train-MSE-0.0000, Train-L2-0.0112, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-256, Time-0.2774, Train-MSE-0.0000, Train-L2-0.0112, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-257, Time-0.3020, Train-MSE-0.0000, Train-L2-0.0112, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-258, Time-0.3194, Train-MSE-0.0000, Train-L2-0.0110, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-259, Time-0.3108, Train-MSE-0.0000, Train-L2-0.0109, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-260, Time-0.3173, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-261, Time-0.3049, Train-MSE-0.0000, Train-L2-0.0108, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-262, Time-0.2903, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-263, Time-0.2937, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-264, Time-0.2896, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-265, Time-0.2804, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-266, Time-0.3146, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-267, Time-0.3201, Train-MSE-0.0000, Train-L2-0.0105, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-268, Time-0.3089, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-269, Time-0.2981, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-270, Time-0.3185, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-271, Time-0.3125, Train-MSE-0.0000, Train-L2-0.0105, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-272, Time-0.3014, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-273, Time-0.2863, Train-MSE-0.0000, Train-L2-0.0105, Test-L2-0.7567, Test-MSE-0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-274, Time-0.2661, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-275, Time-0.2648, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-276, Time-0.2662, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-277, Time-0.2655, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-278, Time-0.2657, Train-MSE-0.0000, Train-L2-0.0103, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-279, Time-0.2649, Train-MSE-0.0000, Train-L2-0.0105, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-280, Time-0.2675, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-281, Time-0.2703, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-282, Time-0.2729, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-283, Time-0.2684, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-284, Time-0.2675, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-285, Time-0.2673, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-286, Time-0.2683, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-287, Time-0.2758, Train-MSE-0.0000, Train-L2-0.0108, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-288, Time-0.2716, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-289, Time-0.2650, Train-MSE-0.0000, Train-L2-0.0108, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-290, Time-0.2651, Train-MSE-0.0000, Train-L2-0.0110, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-291, Time-0.2727, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7566, Test-MSE-0.0009\n",
      "Epoch-292, Time-0.2682, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7572, Test-MSE-0.0009\n",
      "Epoch-293, Time-0.2647, Train-MSE-0.0000, Train-L2-0.0106, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-294, Time-0.2906, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-295, Time-0.3083, Train-MSE-0.0000, Train-L2-0.0104, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-296, Time-0.3107, Train-MSE-0.0000, Train-L2-0.0110, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-297, Time-0.3070, Train-MSE-0.0000, Train-L2-0.0109, Test-L2-0.7565, Test-MSE-0.0009\n",
      "Epoch-298, Time-0.3022, Train-MSE-0.0000, Train-L2-0.0108, Test-L2-0.7576, Test-MSE-0.0009\n",
      "Epoch-299, Time-0.3217, Train-MSE-0.0000, Train-L2-0.0112, Test-L2-0.7561, Test-MSE-0.0009\n",
      "Epoch-300, Time-0.3017, Train-MSE-0.0000, Train-L2-0.0111, Test-L2-0.7562, Test-MSE-0.0009\n",
      "Epoch-301, Time-0.2964, Train-MSE-0.0000, Train-L2-0.0107, Test-L2-0.7567, Test-MSE-0.0009\n",
      "Epoch-302, Time-0.2987, Train-MSE-0.0000, Train-L2-0.0102, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-303, Time-0.3184, Train-MSE-0.0000, Train-L2-0.0100, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-304, Time-0.3051, Train-MSE-0.0000, Train-L2-0.0098, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-305, Time-0.2752, Train-MSE-0.0000, Train-L2-0.0097, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-306, Time-0.3101, Train-MSE-0.0000, Train-L2-0.0097, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-307, Time-0.3120, Train-MSE-0.0000, Train-L2-0.0096, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-308, Time-0.3049, Train-MSE-0.0000, Train-L2-0.0096, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-309, Time-0.2973, Train-MSE-0.0000, Train-L2-0.0096, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-310, Time-0.3066, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-311, Time-0.3154, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-312, Time-0.3154, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-313, Time-0.3251, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-314, Time-0.3121, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-315, Time-0.3137, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-316, Time-0.3253, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7568, Test-MSE-0.0009\n",
      "Epoch-317, Time-0.3061, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-318, Time-0.3015, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-319, Time-0.3141, Train-MSE-0.0000, Train-L2-0.0095, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-320, Time-0.2968, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-321, Time-0.3122, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-322, Time-0.3115, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-323, Time-0.3117, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-324, Time-0.3013, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-325, Time-0.3088, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-326, Time-0.2967, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-327, Time-0.3084, Train-MSE-0.0000, Train-L2-0.0094, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-328, Time-0.3039, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-329, Time-0.3090, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-330, Time-0.3202, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-331, Time-0.3121, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-332, Time-0.3080, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-333, Time-0.3160, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-334, Time-0.3308, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-335, Time-0.3102, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-336, Time-0.3004, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-337, Time-0.3073, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-338, Time-0.3091, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-339, Time-0.3023, Train-MSE-0.0000, Train-L2-0.0093, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-340, Time-0.2967, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-341, Time-0.3058, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-342, Time-0.2652, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-343, Time-0.2644, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-344, Time-0.2667, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-345, Time-0.2658, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-346, Time-0.2659, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-347, Time-0.2652, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-348, Time-0.2652, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7569, Test-MSE-0.0009\n",
      "Epoch-349, Time-0.2632, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-350, Time-0.2662, Train-MSE-0.0000, Train-L2-0.0092, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-351, Time-0.2646, Train-MSE-0.0000, Train-L2-0.0091, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-352, Time-0.2638, Train-MSE-0.0000, Train-L2-0.0091, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-353, Time-0.2649, Train-MSE-0.0000, Train-L2-0.0091, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-354, Time-0.2654, Train-MSE-0.0000, Train-L2-0.0091, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-355, Time-0.2600, Train-MSE-0.0000, Train-L2-0.0091, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-356, Time-0.2510, Train-MSE-0.0000, Train-L2-0.0091, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-357, Time-0.2779, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-358, Time-0.2908, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-359, Time-0.2895, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-360, Time-0.2961, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-361, Time-0.2856, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-362, Time-0.2894, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-363, Time-0.2844, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-364, Time-0.2819, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-365, Time-0.3048, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-366, Time-0.2965, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-367, Time-0.2900, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-368, Time-0.2936, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-369, Time-0.3087, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-370, Time-0.3029, Train-MSE-0.0000, Train-L2-0.0090, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-371, Time-0.2966, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-372, Time-0.3119, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-373, Time-0.3181, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-374, Time-0.3009, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-375, Time-0.3010, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-376, Time-0.2988, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-377, Time-0.2998, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-378, Time-0.3032, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-379, Time-0.3020, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-380, Time-0.3063, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-381, Time-0.3008, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-382, Time-0.2980, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-383, Time-0.3064, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-384, Time-0.3010, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-385, Time-0.2959, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-386, Time-0.3046, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-387, Time-0.3030, Train-MSE-0.0000, Train-L2-0.0089, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-388, Time-0.3122, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-389, Time-0.3092, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-390, Time-0.3127, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-391, Time-0.3194, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-392, Time-0.3126, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-393, Time-0.3062, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-394, Time-0.3094, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-395, Time-0.3119, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-396, Time-0.3128, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-397, Time-0.3079, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-398, Time-0.3083, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7570, Test-MSE-0.0009\n",
      "Epoch-399, Time-0.3155, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-400, Time-0.3108, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-401, Time-0.2969, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-402, Time-0.3122, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-403, Time-0.3014, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-404, Time-0.3157, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-405, Time-0.3079, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-406, Time-0.2978, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-407, Time-0.3094, Train-MSE-0.0000, Train-L2-0.0088, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-408, Time-0.3185, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-409, Time-0.2999, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-410, Time-0.3063, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-411, Time-0.3108, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-412, Time-0.3100, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-413, Time-0.3237, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-414, Time-0.3135, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-415, Time-0.3031, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-416, Time-0.3162, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-417, Time-0.2967, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-418, Time-0.3078, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-419, Time-0.2995, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-420, Time-0.3013, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-421, Time-0.2965, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-422, Time-0.3021, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-423, Time-0.3032, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-424, Time-0.3113, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-425, Time-0.3209, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-426, Time-0.3054, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-427, Time-0.3180, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-428, Time-0.3073, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-429, Time-0.3162, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-430, Time-0.3116, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-431, Time-0.3034, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-432, Time-0.2836, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-433, Time-0.2953, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-434, Time-0.2899, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-435, Time-0.3002, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-436, Time-0.2969, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-437, Time-0.3091, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-438, Time-0.3001, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-439, Time-0.3108, Train-MSE-0.0000, Train-L2-0.0087, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-440, Time-0.3079, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-441, Time-0.3118, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-442, Time-0.3067, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-443, Time-0.3115, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-444, Time-0.3138, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-445, Time-0.3022, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-446, Time-0.3117, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-447, Time-0.2999, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-448, Time-0.3072, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-449, Time-0.3002, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-450, Time-0.3223, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-451, Time-0.2968, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-452, Time-0.3099, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-453, Time-0.3152, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-454, Time-0.3013, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-455, Time-0.3121, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-456, Time-0.3055, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-457, Time-0.3093, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-458, Time-0.2995, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-459, Time-0.3090, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-460, Time-0.3086, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-461, Time-0.3089, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-462, Time-0.3064, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-463, Time-0.3066, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-464, Time-0.3087, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-465, Time-0.3256, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-466, Time-0.2847, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-467, Time-0.3000, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-468, Time-0.3035, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-469, Time-0.3082, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-470, Time-0.2874, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-471, Time-0.3160, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-472, Time-0.3073, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-473, Time-0.3027, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-474, Time-0.3066, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-475, Time-0.3019, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-476, Time-0.2924, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-477, Time-0.3051, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-478, Time-0.3126, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-479, Time-0.3138, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-480, Time-0.3093, Train-MSE-0.0000, Train-L2-0.0086, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-481, Time-0.3098, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-482, Time-0.3102, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-483, Time-0.2952, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-484, Time-0.2984, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-485, Time-0.3058, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-486, Time-0.3031, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-487, Time-0.2957, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-488, Time-0.3007, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-489, Time-0.2916, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-490, Time-0.3233, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-491, Time-0.3103, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-492, Time-0.2977, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-493, Time-0.3170, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-494, Time-0.3090, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-495, Time-0.3098, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-496, Time-0.3234, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-497, Time-0.2965, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-498, Time-0.3058, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n",
      "Epoch-499, Time-0.3161, Train-MSE-0.0000, Train-L2-0.0085, Test-L2-0.7571, Test-MSE-0.0009\n"
     ]
    }
   ],
   "source": [
    "myloss = LpLoss(size_average=False)\n",
    "for ep in range(epochs):\n",
    "    model_mf.train()\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader_mf:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_mf(x)\n",
    "\n",
    "        mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "        l2.backward() # use the l2 relative loss\n",
    "\n",
    "        optimizer.step()\n",
    "        train_mse += mse.item()\n",
    "        train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    model_mf.eval()\n",
    "    test_l2 = 0.0\n",
    "    test_mse = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader_mf:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model_mf(x)\n",
    "            test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "            tmse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "            test_mse += tmse.item()\n",
    "\n",
    "    train_mse /= len(train_loader_mf)\n",
    "    train_l2 /= ntrain\n",
    "    test_l2 /= ntest\n",
    "    test_mse /= len(test_loader_mf)\n",
    "    t2 = default_timer()\n",
    "    print('Epoch-{}, Time-{:0.4f}, Train-MSE-{:0.4f}, Train-L2-{:0.4f}, Test-L2-{:0.4f}, Test-MSE-{:0.4f}'\n",
    "          .format(ep, t2-t1, train_mse, train_l2, test_l2, test_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MF-WNO model\n",
    "\n",
    "torch.save(model_mf, 'model/MF_WNO_poisson1D_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1658147955708,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "iz6udzkEUOv6"
   },
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "pred_mf = []\n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    for x, y in test_loader_mf:\n",
    "        test_l2 = 0 \n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        out = model_mf(x).squeeze(-1)\n",
    "        test_l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "        pred_mf.append( out.cpu() )\n",
    "        print(\"Batch-{}, Test-loss-{:0.6f}\".format( index, test_l2 ))\n",
    "        index += 1\n",
    "\n",
    "pred_mf = torch.cat(( pred_mf ))\n",
    "print('Mean mse_mf-{}'.format(F.mse_loss(y_test_mf, pred_mf).item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1658147955709,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "zoJ53LM7gFk0"
   },
   "outputs": [],
   "source": [
    "inp_mf  = x_test_mf \n",
    "real_mf = y_test_mf + inp_mf[:,:,1]\n",
    "output_mf  =  pred_mf + inp_mf[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_pred = F.mse_loss(output_mf, real_mf).item()\n",
    "\n",
    "print('MSE-Predicted solution-{:0.6f}'.format(mse_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 1396,
     "status": "ok",
     "timestamp": 1658147957091,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "S3uABam-fxJW",
    "outputId": "3a13c252-c85c-4acc-948f-5d428be261c3"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman' \n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "\n",
    "colormap = plt.cm.jet  \n",
    "colors = [colormap(i) for i in np.linspace(0, 1, 5)]\n",
    "\n",
    "fig2 = plt.figure(figsize = (10, 4), dpi=300)\n",
    "fig2.suptitle('Stochastic Heat - FNO - High fidelity')\n",
    "\n",
    "index = 0\n",
    "for i in range(ntest):\n",
    "    if i % 10 == 1:\n",
    "        plt.plot(x_coords, real_mf[i, :], color=colors[index], label='Actual')\n",
    "        plt.plot(x_coords, output_mf[i,:], '--', color=colors[index], label='Prediction')\n",
    "        index += 1\n",
    "plt.legend(ncol=4, loc=4, labelspacing=0.25, columnspacing=0.25, handletextpad=0.5, handlelength=1)\n",
    "plt.grid(True)\n",
    "plt.margins(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1658147958337,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "YWN6Y0-M-J3i",
    "outputId": "dd5a76ff-479e-4883-8dce-b219ad250ffd"
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.jet  \n",
    "colors2 = [colormap(i) for i in np.linspace(0, 1, 5)]\n",
    "\n",
    "fig1 = plt.figure(figsize = (10, 4), dpi=300)\n",
    "fig1.suptitle('Stochastic Heat - FNO - High fidelity')\n",
    "\n",
    "index = 0\n",
    "for i in range(ntest):\n",
    "    if i % 10 == 1:\n",
    "        plt.plot(x_coords, inp_mf[i, :, 0], color=colors2[index], label='Forcing-{}'.format(i))\n",
    "        index += 1\n",
    "plt.legend(ncol=4, loc=4, labelspacing=0.25, columnspacing=0.25, handletextpad=0.5, handlelength=1)\n",
    "plt.grid(True)\n",
    "plt.margins(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1658147958348,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "OLqVEOUVIK9_",
    "outputId": "becdfed1-3ea5-4a0e-8f74-054bc96ce2b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdlYHIgzRr4D"
   },
   "source": [
    "# High Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "x_hf = torch.tensor( x_data_h, dtype=torch.float ).unsqueeze(-1)\n",
    "y_hf = torch.tensor( y_data_h, dtype=torch.float ) \n",
    "\n",
    "x_train_hf, y_train_hf = x_hf[:ntrain, ...], y_hf[:ntrain, ...]\n",
    "x_test_hf, y_test_hf = x_hf[-ntest:, ...], y_hf[-ntest:, ...]\n",
    "\n",
    "train_loader_hf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_hf, y_train_hf),\n",
    "                                              batch_size=batch_size, shuffle=True)\n",
    "test_loader_hf = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_hf, x_test_hf),\n",
    "                                             batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_hf.shape, y_train_hf.shape, x_test_hf.shape, y_test_hf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1658147958359,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "tugoovSqS1MN",
    "outputId": "f9eaff01-a1ef-4465-a1be-440dea3ce133"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_hf = WNO1d(width=width, level=4, size=h, wavelet=wavelet,\n",
    "              in_channel=2, grid_range=grid_range).to(device)\n",
    "print(count_params(model_hf))\n",
    "\n",
    "optimizer = torch.optim.Adam(model_hf.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114187,
     "status": "ok",
     "timestamp": 1658148072492,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "_TdWb1seS1MP",
    "outputId": "8681d9f6-568d-44a1-9c3e-a185c106d3ae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-113, Time-0.3040, Train-MSE-0.0214, Train-L2-0.0936, Test-L2-1.5716\n",
      "Epoch-114, Time-0.3016, Train-MSE-0.0217, Train-L2-0.0930, Test-L2-1.5987\n",
      "Epoch-115, Time-0.3182, Train-MSE-0.0222, Train-L2-0.0952, Test-L2-1.5748\n",
      "Epoch-116, Time-0.3038, Train-MSE-0.0215, Train-L2-0.0951, Test-L2-1.6041\n",
      "Epoch-117, Time-0.3100, Train-MSE-0.0190, Train-L2-0.1013, Test-L2-1.5706\n",
      "Epoch-118, Time-0.3050, Train-MSE-0.0195, Train-L2-0.0842, Test-L2-1.5894\n",
      "Epoch-119, Time-0.2956, Train-MSE-0.0181, Train-L2-0.0885, Test-L2-1.5819\n",
      "Epoch-120, Time-0.3044, Train-MSE-0.0194, Train-L2-0.0861, Test-L2-1.5924\n",
      "Epoch-121, Time-0.3107, Train-MSE-0.0169, Train-L2-0.0857, Test-L2-1.5975\n",
      "Epoch-122, Time-0.3054, Train-MSE-0.0189, Train-L2-0.0826, Test-L2-1.5811\n",
      "Epoch-123, Time-0.3002, Train-MSE-0.0153, Train-L2-0.0811, Test-L2-1.5979\n",
      "Epoch-124, Time-0.3150, Train-MSE-0.0162, Train-L2-0.0806, Test-L2-1.5888\n",
      "Epoch-125, Time-0.2918, Train-MSE-0.0165, Train-L2-0.0802, Test-L2-1.6065\n",
      "Epoch-126, Time-0.2967, Train-MSE-0.0165, Train-L2-0.0749, Test-L2-1.5831\n",
      "Epoch-127, Time-0.2952, Train-MSE-0.0157, Train-L2-0.0711, Test-L2-1.6012\n",
      "Epoch-128, Time-0.3022, Train-MSE-0.0140, Train-L2-0.0701, Test-L2-1.5972\n",
      "Epoch-129, Time-0.3021, Train-MSE-0.0144, Train-L2-0.0696, Test-L2-1.5897\n",
      "Epoch-130, Time-0.3039, Train-MSE-0.0136, Train-L2-0.0703, Test-L2-1.6022\n",
      "Epoch-131, Time-0.2808, Train-MSE-0.0140, Train-L2-0.0783, Test-L2-1.5883\n",
      "Epoch-132, Time-0.2985, Train-MSE-0.0128, Train-L2-0.0821, Test-L2-1.6062\n",
      "Epoch-133, Time-0.3106, Train-MSE-0.0131, Train-L2-0.0926, Test-L2-1.5953\n",
      "Epoch-134, Time-0.3172, Train-MSE-0.0122, Train-L2-0.0986, Test-L2-1.5928\n",
      "Epoch-135, Time-0.3100, Train-MSE-0.0162, Train-L2-0.1071, Test-L2-1.5974\n",
      "Epoch-136, Time-0.3161, Train-MSE-0.0108, Train-L2-0.0850, Test-L2-1.5981\n",
      "Epoch-137, Time-0.3078, Train-MSE-0.0116, Train-L2-0.0798, Test-L2-1.5895\n",
      "Epoch-138, Time-0.3081, Train-MSE-0.0106, Train-L2-0.0805, Test-L2-1.5924\n",
      "Epoch-139, Time-0.3070, Train-MSE-0.0114, Train-L2-0.0804, Test-L2-1.6008\n",
      "Epoch-140, Time-0.3008, Train-MSE-0.0099, Train-L2-0.0841, Test-L2-1.5975\n",
      "Epoch-141, Time-0.3151, Train-MSE-0.0147, Train-L2-0.1168, Test-L2-1.5777\n",
      "Epoch-142, Time-0.3053, Train-MSE-0.0111, Train-L2-0.1249, Test-L2-1.6142\n",
      "Epoch-143, Time-0.3050, Train-MSE-0.0104, Train-L2-0.1005, Test-L2-1.5645\n",
      "Epoch-144, Time-0.3068, Train-MSE-0.0107, Train-L2-0.0965, Test-L2-1.5881\n",
      "Epoch-145, Time-0.2978, Train-MSE-0.0101, Train-L2-0.0935, Test-L2-1.5896\n",
      "Epoch-146, Time-0.3092, Train-MSE-0.0095, Train-L2-0.0747, Test-L2-1.6041\n",
      "Epoch-147, Time-0.3034, Train-MSE-0.0090, Train-L2-0.0722, Test-L2-1.5981\n",
      "Epoch-148, Time-0.2765, Train-MSE-0.0079, Train-L2-0.0728, Test-L2-1.5798\n",
      "Epoch-149, Time-0.2719, Train-MSE-0.0074, Train-L2-0.0648, Test-L2-1.6167\n",
      "Epoch-150, Time-0.2702, Train-MSE-0.0073, Train-L2-0.0713, Test-L2-1.5984\n",
      "Epoch-151, Time-0.2694, Train-MSE-0.0081, Train-L2-0.0624, Test-L2-1.5890\n",
      "Epoch-152, Time-0.2704, Train-MSE-0.0071, Train-L2-0.0594, Test-L2-1.6002\n",
      "Epoch-153, Time-0.2690, Train-MSE-0.0078, Train-L2-0.0616, Test-L2-1.5925\n",
      "Epoch-154, Time-0.2703, Train-MSE-0.0078, Train-L2-0.0575, Test-L2-1.5941\n",
      "Epoch-155, Time-0.2694, Train-MSE-0.0074, Train-L2-0.0587, Test-L2-1.5928\n",
      "Epoch-156, Time-0.2693, Train-MSE-0.0076, Train-L2-0.0601, Test-L2-1.5939\n",
      "Epoch-157, Time-0.2682, Train-MSE-0.0070, Train-L2-0.0570, Test-L2-1.5950\n",
      "Epoch-158, Time-0.2648, Train-MSE-0.0068, Train-L2-0.0567, Test-L2-1.6011\n",
      "Epoch-159, Time-0.2607, Train-MSE-0.0061, Train-L2-0.0625, Test-L2-1.5985\n",
      "Epoch-160, Time-0.2681, Train-MSE-0.0069, Train-L2-0.0612, Test-L2-1.5985\n",
      "Epoch-161, Time-0.2945, Train-MSE-0.0059, Train-L2-0.0589, Test-L2-1.5998\n",
      "Epoch-162, Time-0.2965, Train-MSE-0.0069, Train-L2-0.0573, Test-L2-1.5916\n",
      "Epoch-163, Time-0.2982, Train-MSE-0.0061, Train-L2-0.0471, Test-L2-1.5968\n",
      "Epoch-164, Time-0.3109, Train-MSE-0.0059, Train-L2-0.0458, Test-L2-1.5963\n",
      "Epoch-165, Time-0.2995, Train-MSE-0.0060, Train-L2-0.0480, Test-L2-1.5985\n",
      "Epoch-166, Time-0.3041, Train-MSE-0.0055, Train-L2-0.0484, Test-L2-1.6008\n",
      "Epoch-167, Time-0.3090, Train-MSE-0.0057, Train-L2-0.0475, Test-L2-1.5925\n",
      "Epoch-168, Time-0.3062, Train-MSE-0.0056, Train-L2-0.0441, Test-L2-1.6003\n",
      "Epoch-169, Time-0.3071, Train-MSE-0.0055, Train-L2-0.0438, Test-L2-1.5922\n",
      "Epoch-170, Time-0.3119, Train-MSE-0.0057, Train-L2-0.0431, Test-L2-1.5989\n",
      "Epoch-171, Time-0.2787, Train-MSE-0.0055, Train-L2-0.0439, Test-L2-1.5947\n",
      "Epoch-172, Time-0.2880, Train-MSE-0.0054, Train-L2-0.0426, Test-L2-1.5970\n",
      "Epoch-173, Time-0.2881, Train-MSE-0.0052, Train-L2-0.0452, Test-L2-1.6001\n",
      "Epoch-174, Time-0.3093, Train-MSE-0.0051, Train-L2-0.0406, Test-L2-1.5973\n",
      "Epoch-175, Time-0.3144, Train-MSE-0.0049, Train-L2-0.0399, Test-L2-1.5941\n",
      "Epoch-176, Time-0.3062, Train-MSE-0.0050, Train-L2-0.0406, Test-L2-1.6014\n",
      "Epoch-177, Time-0.2910, Train-MSE-0.0048, Train-L2-0.0398, Test-L2-1.5977\n",
      "Epoch-178, Time-0.2999, Train-MSE-0.0046, Train-L2-0.0395, Test-L2-1.5947\n",
      "Epoch-179, Time-0.3107, Train-MSE-0.0046, Train-L2-0.0389, Test-L2-1.5969\n",
      "Epoch-180, Time-0.3006, Train-MSE-0.0050, Train-L2-0.0450, Test-L2-1.6000\n",
      "Epoch-181, Time-0.3009, Train-MSE-0.0042, Train-L2-0.0448, Test-L2-1.5977\n",
      "Epoch-182, Time-0.2997, Train-MSE-0.0044, Train-L2-0.0470, Test-L2-1.5950\n",
      "Epoch-183, Time-0.2997, Train-MSE-0.0043, Train-L2-0.0439, Test-L2-1.6023\n",
      "Epoch-184, Time-0.3041, Train-MSE-0.0044, Train-L2-0.0425, Test-L2-1.5917\n",
      "Epoch-185, Time-0.3022, Train-MSE-0.0044, Train-L2-0.0407, Test-L2-1.6008\n",
      "Epoch-186, Time-0.2975, Train-MSE-0.0041, Train-L2-0.0438, Test-L2-1.5930\n",
      "Epoch-187, Time-0.2872, Train-MSE-0.0044, Train-L2-0.0417, Test-L2-1.5951\n",
      "Epoch-188, Time-0.3064, Train-MSE-0.0045, Train-L2-0.0433, Test-L2-1.6009\n",
      "Epoch-189, Time-0.2994, Train-MSE-0.0040, Train-L2-0.0435, Test-L2-1.5964\n",
      "Epoch-190, Time-0.2904, Train-MSE-0.0041, Train-L2-0.0374, Test-L2-1.5970\n",
      "Epoch-191, Time-0.2898, Train-MSE-0.0039, Train-L2-0.0382, Test-L2-1.5951\n",
      "Epoch-192, Time-0.3016, Train-MSE-0.0040, Train-L2-0.0393, Test-L2-1.5996\n",
      "Epoch-193, Time-0.2945, Train-MSE-0.0039, Train-L2-0.0419, Test-L2-1.5936\n",
      "Epoch-194, Time-0.3033, Train-MSE-0.0039, Train-L2-0.0390, Test-L2-1.6049\n",
      "Epoch-195, Time-0.2991, Train-MSE-0.0039, Train-L2-0.0384, Test-L2-1.5914\n",
      "Epoch-196, Time-0.2919, Train-MSE-0.0038, Train-L2-0.0396, Test-L2-1.6034\n",
      "Epoch-197, Time-0.3029, Train-MSE-0.0036, Train-L2-0.0445, Test-L2-1.5955\n",
      "Epoch-198, Time-0.3033, Train-MSE-0.0037, Train-L2-0.0396, Test-L2-1.6000\n",
      "Epoch-199, Time-0.2986, Train-MSE-0.0034, Train-L2-0.0413, Test-L2-1.5958\n",
      "Epoch-200, Time-0.3063, Train-MSE-0.0038, Train-L2-0.0369, Test-L2-1.5968\n",
      "Epoch-201, Time-0.3101, Train-MSE-0.0037, Train-L2-0.0362, Test-L2-1.5978\n",
      "Epoch-202, Time-0.3014, Train-MSE-0.0033, Train-L2-0.0359, Test-L2-1.6008\n",
      "Epoch-203, Time-0.2961, Train-MSE-0.0035, Train-L2-0.0359, Test-L2-1.5918\n",
      "Epoch-204, Time-0.3002, Train-MSE-0.0034, Train-L2-0.0364, Test-L2-1.5982\n",
      "Epoch-205, Time-0.3062, Train-MSE-0.0034, Train-L2-0.0360, Test-L2-1.6003\n",
      "Epoch-206, Time-0.3007, Train-MSE-0.0036, Train-L2-0.0368, Test-L2-1.5962\n",
      "Epoch-207, Time-0.2984, Train-MSE-0.0033, Train-L2-0.0373, Test-L2-1.5971\n",
      "Epoch-208, Time-0.2939, Train-MSE-0.0036, Train-L2-0.0387, Test-L2-1.5974\n",
      "Epoch-209, Time-0.3098, Train-MSE-0.0033, Train-L2-0.0346, Test-L2-1.5981\n",
      "Epoch-210, Time-0.3009, Train-MSE-0.0034, Train-L2-0.0351, Test-L2-1.5936\n",
      "Epoch-211, Time-0.3039, Train-MSE-0.0032, Train-L2-0.0331, Test-L2-1.5981\n",
      "Epoch-212, Time-0.2882, Train-MSE-0.0031, Train-L2-0.0355, Test-L2-1.5938\n",
      "Epoch-213, Time-0.3086, Train-MSE-0.0033, Train-L2-0.0378, Test-L2-1.5985\n",
      "Epoch-214, Time-0.3056, Train-MSE-0.0030, Train-L2-0.0320, Test-L2-1.5979\n",
      "Epoch-215, Time-0.2997, Train-MSE-0.0032, Train-L2-0.0325, Test-L2-1.5974\n",
      "Epoch-216, Time-0.3021, Train-MSE-0.0030, Train-L2-0.0315, Test-L2-1.5983\n",
      "Epoch-217, Time-0.2960, Train-MSE-0.0030, Train-L2-0.0317, Test-L2-1.5976\n",
      "Epoch-218, Time-0.3025, Train-MSE-0.0030, Train-L2-0.0314, Test-L2-1.5981\n",
      "Epoch-219, Time-0.2957, Train-MSE-0.0030, Train-L2-0.0299, Test-L2-1.5964\n",
      "Epoch-220, Time-0.2966, Train-MSE-0.0030, Train-L2-0.0297, Test-L2-1.5964\n",
      "Epoch-221, Time-0.2965, Train-MSE-0.0030, Train-L2-0.0307, Test-L2-1.5995\n",
      "Epoch-222, Time-0.3011, Train-MSE-0.0030, Train-L2-0.0310, Test-L2-1.5957\n",
      "Epoch-223, Time-0.3018, Train-MSE-0.0029, Train-L2-0.0327, Test-L2-1.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-224, Time-0.2991, Train-MSE-0.0031, Train-L2-0.0359, Test-L2-1.5954\n",
      "Epoch-225, Time-0.3067, Train-MSE-0.0031, Train-L2-0.0366, Test-L2-1.5974\n",
      "Epoch-226, Time-0.2994, Train-MSE-0.0029, Train-L2-0.0338, Test-L2-1.5968\n",
      "Epoch-227, Time-0.3031, Train-MSE-0.0032, Train-L2-0.0365, Test-L2-1.5998\n",
      "Epoch-228, Time-0.2912, Train-MSE-0.0028, Train-L2-0.0367, Test-L2-1.5962\n",
      "Epoch-229, Time-0.2863, Train-MSE-0.0029, Train-L2-0.0319, Test-L2-1.5974\n",
      "Epoch-230, Time-0.2868, Train-MSE-0.0028, Train-L2-0.0313, Test-L2-1.5964\n",
      "Epoch-231, Time-0.2747, Train-MSE-0.0029, Train-L2-0.0317, Test-L2-1.5989\n",
      "Epoch-232, Time-0.3004, Train-MSE-0.0029, Train-L2-0.0309, Test-L2-1.5971\n",
      "Epoch-233, Time-0.3017, Train-MSE-0.0029, Train-L2-0.0325, Test-L2-1.5968\n",
      "Epoch-234, Time-0.2858, Train-MSE-0.0029, Train-L2-0.0330, Test-L2-1.5960\n",
      "Epoch-235, Time-0.3116, Train-MSE-0.0030, Train-L2-0.0366, Test-L2-1.6011\n",
      "Epoch-236, Time-0.2988, Train-MSE-0.0027, Train-L2-0.0335, Test-L2-1.5936\n",
      "Epoch-237, Time-0.3007, Train-MSE-0.0027, Train-L2-0.0296, Test-L2-1.5986\n",
      "Epoch-238, Time-0.3145, Train-MSE-0.0026, Train-L2-0.0295, Test-L2-1.5977\n",
      "Epoch-239, Time-0.3023, Train-MSE-0.0028, Train-L2-0.0304, Test-L2-1.5969\n",
      "Epoch-240, Time-0.3063, Train-MSE-0.0026, Train-L2-0.0297, Test-L2-1.5999\n",
      "Epoch-241, Time-0.3067, Train-MSE-0.0028, Train-L2-0.0317, Test-L2-1.5956\n",
      "Epoch-242, Time-0.3092, Train-MSE-0.0028, Train-L2-0.0315, Test-L2-1.5931\n",
      "Epoch-243, Time-0.3066, Train-MSE-0.0029, Train-L2-0.0377, Test-L2-1.6035\n",
      "Epoch-244, Time-0.3036, Train-MSE-0.0026, Train-L2-0.0427, Test-L2-1.5955\n",
      "Epoch-245, Time-0.3121, Train-MSE-0.0026, Train-L2-0.0360, Test-L2-1.5945\n",
      "Epoch-246, Time-0.2964, Train-MSE-0.0025, Train-L2-0.0338, Test-L2-1.5998\n",
      "Epoch-247, Time-0.2961, Train-MSE-0.0027, Train-L2-0.0325, Test-L2-1.5992\n",
      "Epoch-248, Time-0.2917, Train-MSE-0.0026, Train-L2-0.0314, Test-L2-1.5996\n",
      "Epoch-249, Time-0.2911, Train-MSE-0.0025, Train-L2-0.0303, Test-L2-1.5938\n",
      "Epoch-250, Time-0.2873, Train-MSE-0.0027, Train-L2-0.0302, Test-L2-1.5961\n",
      "Epoch-251, Time-0.2682, Train-MSE-0.0025, Train-L2-0.0268, Test-L2-1.5968\n",
      "Epoch-252, Time-0.2852, Train-MSE-0.0025, Train-L2-0.0264, Test-L2-1.5978\n",
      "Epoch-253, Time-0.2758, Train-MSE-0.0024, Train-L2-0.0256, Test-L2-1.5968\n",
      "Epoch-254, Time-0.2832, Train-MSE-0.0025, Train-L2-0.0252, Test-L2-1.5967\n",
      "Epoch-255, Time-0.2963, Train-MSE-0.0024, Train-L2-0.0249, Test-L2-1.5974\n",
      "Epoch-256, Time-0.3084, Train-MSE-0.0025, Train-L2-0.0252, Test-L2-1.5960\n",
      "Epoch-257, Time-0.3019, Train-MSE-0.0024, Train-L2-0.0253, Test-L2-1.5971\n",
      "Epoch-258, Time-0.2966, Train-MSE-0.0024, Train-L2-0.0250, Test-L2-1.5968\n",
      "Epoch-259, Time-0.2884, Train-MSE-0.0024, Train-L2-0.0250, Test-L2-1.5960\n",
      "Epoch-260, Time-0.2861, Train-MSE-0.0024, Train-L2-0.0251, Test-L2-1.5975\n",
      "Epoch-261, Time-0.2962, Train-MSE-0.0024, Train-L2-0.0252, Test-L2-1.5965\n",
      "Epoch-262, Time-0.2994, Train-MSE-0.0025, Train-L2-0.0249, Test-L2-1.5981\n",
      "Epoch-263, Time-0.2898, Train-MSE-0.0024, Train-L2-0.0249, Test-L2-1.5965\n",
      "Epoch-264, Time-0.2949, Train-MSE-0.0024, Train-L2-0.0256, Test-L2-1.5972\n",
      "Epoch-265, Time-0.2901, Train-MSE-0.0024, Train-L2-0.0255, Test-L2-1.5970\n",
      "Epoch-266, Time-0.2994, Train-MSE-0.0024, Train-L2-0.0247, Test-L2-1.5975\n",
      "Epoch-267, Time-0.3072, Train-MSE-0.0024, Train-L2-0.0248, Test-L2-1.5961\n",
      "Epoch-268, Time-0.3218, Train-MSE-0.0024, Train-L2-0.0250, Test-L2-1.5981\n",
      "Epoch-269, Time-0.3064, Train-MSE-0.0023, Train-L2-0.0251, Test-L2-1.5972\n",
      "Epoch-270, Time-0.3068, Train-MSE-0.0024, Train-L2-0.0252, Test-L2-1.5968\n",
      "Epoch-271, Time-0.3045, Train-MSE-0.0023, Train-L2-0.0249, Test-L2-1.5977\n",
      "Epoch-272, Time-0.3106, Train-MSE-0.0023, Train-L2-0.0246, Test-L2-1.5976\n",
      "Epoch-273, Time-0.2986, Train-MSE-0.0023, Train-L2-0.0243, Test-L2-1.5952\n",
      "Epoch-274, Time-0.3016, Train-MSE-0.0024, Train-L2-0.0244, Test-L2-1.5987\n",
      "Epoch-275, Time-0.2959, Train-MSE-0.0023, Train-L2-0.0251, Test-L2-1.5956\n",
      "Epoch-276, Time-0.3039, Train-MSE-0.0024, Train-L2-0.0257, Test-L2-1.5977\n",
      "Epoch-277, Time-0.3078, Train-MSE-0.0023, Train-L2-0.0258, Test-L2-1.5975\n",
      "Epoch-278, Time-0.3070, Train-MSE-0.0023, Train-L2-0.0243, Test-L2-1.5972\n",
      "Epoch-279, Time-0.3029, Train-MSE-0.0022, Train-L2-0.0241, Test-L2-1.5972\n",
      "Epoch-280, Time-0.3167, Train-MSE-0.0023, Train-L2-0.0241, Test-L2-1.5974\n",
      "Epoch-281, Time-0.2983, Train-MSE-0.0023, Train-L2-0.0245, Test-L2-1.5973\n",
      "Epoch-282, Time-0.3080, Train-MSE-0.0022, Train-L2-0.0236, Test-L2-1.5963\n",
      "Epoch-283, Time-0.3150, Train-MSE-0.0022, Train-L2-0.0240, Test-L2-1.5977\n",
      "Epoch-284, Time-0.3093, Train-MSE-0.0022, Train-L2-0.0248, Test-L2-1.5974\n",
      "Epoch-285, Time-0.2849, Train-MSE-0.0023, Train-L2-0.0250, Test-L2-1.5964\n",
      "Epoch-286, Time-0.2708, Train-MSE-0.0022, Train-L2-0.0241, Test-L2-1.5975\n",
      "Epoch-287, Time-0.2460, Train-MSE-0.0022, Train-L2-0.0236, Test-L2-1.5967\n",
      "Epoch-288, Time-0.2230, Train-MSE-0.0022, Train-L2-0.0242, Test-L2-1.5961\n",
      "Epoch-289, Time-0.2546, Train-MSE-0.0022, Train-L2-0.0235, Test-L2-1.5981\n",
      "Epoch-290, Time-0.2735, Train-MSE-0.0021, Train-L2-0.0240, Test-L2-1.5962\n",
      "Epoch-291, Time-0.2701, Train-MSE-0.0023, Train-L2-0.0250, Test-L2-1.5966\n",
      "Epoch-292, Time-0.2702, Train-MSE-0.0022, Train-L2-0.0240, Test-L2-1.5982\n",
      "Epoch-293, Time-0.2703, Train-MSE-0.0022, Train-L2-0.0234, Test-L2-1.5962\n",
      "Epoch-294, Time-0.2599, Train-MSE-0.0022, Train-L2-0.0238, Test-L2-1.5961\n",
      "Epoch-295, Time-0.2574, Train-MSE-0.0022, Train-L2-0.0232, Test-L2-1.5985\n",
      "Epoch-296, Time-0.2484, Train-MSE-0.0022, Train-L2-0.0230, Test-L2-1.5954\n",
      "Epoch-297, Time-0.2594, Train-MSE-0.0022, Train-L2-0.0235, Test-L2-1.5970\n",
      "Epoch-298, Time-0.2576, Train-MSE-0.0021, Train-L2-0.0235, Test-L2-1.5972\n",
      "Epoch-299, Time-0.2599, Train-MSE-0.0022, Train-L2-0.0234, Test-L2-1.5972\n",
      "Epoch-300, Time-0.2611, Train-MSE-0.0021, Train-L2-0.0232, Test-L2-1.5968\n",
      "Epoch-301, Time-0.2566, Train-MSE-0.0021, Train-L2-0.0224, Test-L2-1.5968\n",
      "Epoch-302, Time-0.2612, Train-MSE-0.0021, Train-L2-0.0221, Test-L2-1.5976\n",
      "Epoch-303, Time-0.2589, Train-MSE-0.0021, Train-L2-0.0222, Test-L2-1.5977\n",
      "Epoch-304, Time-0.2491, Train-MSE-0.0021, Train-L2-0.0222, Test-L2-1.5973\n",
      "Epoch-305, Time-0.2694, Train-MSE-0.0021, Train-L2-0.0222, Test-L2-1.5966\n",
      "Epoch-306, Time-0.2717, Train-MSE-0.0021, Train-L2-0.0221, Test-L2-1.5973\n",
      "Epoch-307, Time-0.2707, Train-MSE-0.0021, Train-L2-0.0220, Test-L2-1.5973\n",
      "Epoch-308, Time-0.2725, Train-MSE-0.0021, Train-L2-0.0219, Test-L2-1.5970\n",
      "Epoch-309, Time-0.3089, Train-MSE-0.0021, Train-L2-0.0220, Test-L2-1.5971\n",
      "Epoch-310, Time-0.3104, Train-MSE-0.0021, Train-L2-0.0218, Test-L2-1.5974\n",
      "Epoch-311, Time-0.3161, Train-MSE-0.0021, Train-L2-0.0218, Test-L2-1.5974\n",
      "Epoch-312, Time-0.3107, Train-MSE-0.0021, Train-L2-0.0218, Test-L2-1.5971\n",
      "Epoch-313, Time-0.3139, Train-MSE-0.0021, Train-L2-0.0218, Test-L2-1.5967\n",
      "Epoch-314, Time-0.2969, Train-MSE-0.0021, Train-L2-0.0217, Test-L2-1.5976\n",
      "Epoch-315, Time-0.3018, Train-MSE-0.0021, Train-L2-0.0217, Test-L2-1.5973\n",
      "Epoch-316, Time-0.3199, Train-MSE-0.0021, Train-L2-0.0217, Test-L2-1.5967\n",
      "Epoch-317, Time-0.3044, Train-MSE-0.0021, Train-L2-0.0217, Test-L2-1.5975\n",
      "Epoch-318, Time-0.2903, Train-MSE-0.0021, Train-L2-0.0216, Test-L2-1.5970\n",
      "Epoch-319, Time-0.3067, Train-MSE-0.0021, Train-L2-0.0216, Test-L2-1.5972\n",
      "Epoch-320, Time-0.3139, Train-MSE-0.0020, Train-L2-0.0216, Test-L2-1.5973\n",
      "Epoch-321, Time-0.3073, Train-MSE-0.0021, Train-L2-0.0218, Test-L2-1.5972\n",
      "Epoch-322, Time-0.3074, Train-MSE-0.0020, Train-L2-0.0219, Test-L2-1.5972\n",
      "Epoch-323, Time-0.3051, Train-MSE-0.0021, Train-L2-0.0215, Test-L2-1.5968\n",
      "Epoch-324, Time-0.2996, Train-MSE-0.0020, Train-L2-0.0216, Test-L2-1.5974\n",
      "Epoch-325, Time-0.2970, Train-MSE-0.0020, Train-L2-0.0216, Test-L2-1.5974\n",
      "Epoch-326, Time-0.3069, Train-MSE-0.0020, Train-L2-0.0215, Test-L2-1.5965\n",
      "Epoch-327, Time-0.3128, Train-MSE-0.0021, Train-L2-0.0216, Test-L2-1.5970\n",
      "Epoch-328, Time-0.2988, Train-MSE-0.0020, Train-L2-0.0217, Test-L2-1.5975\n",
      "Epoch-329, Time-0.2866, Train-MSE-0.0020, Train-L2-0.0216, Test-L2-1.5968\n",
      "Epoch-330, Time-0.2622, Train-MSE-0.0020, Train-L2-0.0214, Test-L2-1.5974\n",
      "Epoch-331, Time-0.2582, Train-MSE-0.0020, Train-L2-0.0213, Test-L2-1.5970\n",
      "Epoch-332, Time-0.2582, Train-MSE-0.0020, Train-L2-0.0212, Test-L2-1.5968\n",
      "Epoch-333, Time-0.2607, Train-MSE-0.0020, Train-L2-0.0212, Test-L2-1.5977\n",
      "Epoch-334, Time-0.2569, Train-MSE-0.0020, Train-L2-0.0211, Test-L2-1.5968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-335, Time-0.2899, Train-MSE-0.0020, Train-L2-0.0211, Test-L2-1.5971\n",
      "Epoch-336, Time-0.2995, Train-MSE-0.0020, Train-L2-0.0212, Test-L2-1.5974\n",
      "Epoch-337, Time-0.2994, Train-MSE-0.0020, Train-L2-0.0213, Test-L2-1.5964\n",
      "Epoch-338, Time-0.3011, Train-MSE-0.0020, Train-L2-0.0214, Test-L2-1.5975\n",
      "Epoch-339, Time-0.2964, Train-MSE-0.0020, Train-L2-0.0216, Test-L2-1.5965\n",
      "Epoch-340, Time-0.3070, Train-MSE-0.0020, Train-L2-0.0216, Test-L2-1.5974\n",
      "Epoch-341, Time-0.3065, Train-MSE-0.0020, Train-L2-0.0215, Test-L2-1.5970\n",
      "Epoch-342, Time-0.2971, Train-MSE-0.0020, Train-L2-0.0214, Test-L2-1.5968\n",
      "Epoch-343, Time-0.3004, Train-MSE-0.0020, Train-L2-0.0213, Test-L2-1.5970\n",
      "Epoch-344, Time-0.3009, Train-MSE-0.0020, Train-L2-0.0210, Test-L2-1.5974\n",
      "Epoch-345, Time-0.2441, Train-MSE-0.0020, Train-L2-0.0209, Test-L2-1.5974\n",
      "Epoch-346, Time-0.2906, Train-MSE-0.0020, Train-L2-0.0209, Test-L2-1.5968\n",
      "Epoch-347, Time-0.3101, Train-MSE-0.0020, Train-L2-0.0210, Test-L2-1.5969\n",
      "Epoch-348, Time-0.2991, Train-MSE-0.0020, Train-L2-0.0208, Test-L2-1.5974\n",
      "Epoch-349, Time-0.2855, Train-MSE-0.0020, Train-L2-0.0209, Test-L2-1.5970\n",
      "Epoch-350, Time-0.2946, Train-MSE-0.0020, Train-L2-0.0207, Test-L2-1.5970\n",
      "Epoch-351, Time-0.2907, Train-MSE-0.0020, Train-L2-0.0207, Test-L2-1.5972\n",
      "Epoch-352, Time-0.3032, Train-MSE-0.0019, Train-L2-0.0207, Test-L2-1.5972\n",
      "Epoch-353, Time-0.3020, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5971\n",
      "Epoch-354, Time-0.3064, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5971\n",
      "Epoch-355, Time-0.2966, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5971\n",
      "Epoch-356, Time-0.3076, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5971\n",
      "Epoch-357, Time-0.3104, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5972\n",
      "Epoch-358, Time-0.3030, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5973\n",
      "Epoch-359, Time-0.2748, Train-MSE-0.0019, Train-L2-0.0206, Test-L2-1.5970\n",
      "Epoch-360, Time-0.2490, Train-MSE-0.0019, Train-L2-0.0205, Test-L2-1.5971\n",
      "Epoch-361, Time-0.2525, Train-MSE-0.0019, Train-L2-0.0205, Test-L2-1.5973\n",
      "Epoch-362, Time-0.2631, Train-MSE-0.0019, Train-L2-0.0205, Test-L2-1.5970\n",
      "Epoch-363, Time-0.2553, Train-MSE-0.0019, Train-L2-0.0205, Test-L2-1.5971\n",
      "Epoch-364, Time-0.2495, Train-MSE-0.0019, Train-L2-0.0205, Test-L2-1.5972\n",
      "Epoch-365, Time-0.2599, Train-MSE-0.0019, Train-L2-0.0205, Test-L2-1.5971\n",
      "Epoch-366, Time-0.2529, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5971\n",
      "Epoch-367, Time-0.2523, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5971\n",
      "Epoch-368, Time-0.2520, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5972\n",
      "Epoch-369, Time-0.2587, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5972\n",
      "Epoch-370, Time-0.2608, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5970\n",
      "Epoch-371, Time-0.2550, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5972\n",
      "Epoch-372, Time-0.2546, Train-MSE-0.0019, Train-L2-0.0204, Test-L2-1.5970\n",
      "Epoch-373, Time-0.2582, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5971\n",
      "Epoch-374, Time-0.2522, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5969\n",
      "Epoch-375, Time-0.2622, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5972\n",
      "Epoch-376, Time-0.2573, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5972\n",
      "Epoch-377, Time-0.2457, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5970\n",
      "Epoch-378, Time-0.2607, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5969\n",
      "Epoch-379, Time-0.2524, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5973\n",
      "Epoch-380, Time-0.2432, Train-MSE-0.0019, Train-L2-0.0203, Test-L2-1.5970\n",
      "Epoch-381, Time-0.2496, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5970\n",
      "Epoch-382, Time-0.2475, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5971\n",
      "Epoch-383, Time-0.2691, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5971\n",
      "Epoch-384, Time-0.2525, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5970\n",
      "Epoch-385, Time-0.3001, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5970\n",
      "Epoch-386, Time-0.2827, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5971\n",
      "Epoch-387, Time-0.2813, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5971\n",
      "Epoch-388, Time-0.2843, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5971\n",
      "Epoch-389, Time-0.2607, Train-MSE-0.0019, Train-L2-0.0202, Test-L2-1.5970\n",
      "Epoch-390, Time-0.2691, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5971\n",
      "Epoch-391, Time-0.2614, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5971\n",
      "Epoch-392, Time-0.2560, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5971\n",
      "Epoch-393, Time-0.2824, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5969\n",
      "Epoch-394, Time-0.2748, Train-MSE-0.0019, Train-L2-0.0200, Test-L2-1.5970\n",
      "Epoch-395, Time-0.2872, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5971\n",
      "Epoch-396, Time-0.2956, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5971\n",
      "Epoch-397, Time-0.2994, Train-MSE-0.0019, Train-L2-0.0201, Test-L2-1.5970\n",
      "Epoch-398, Time-0.3112, Train-MSE-0.0019, Train-L2-0.0200, Test-L2-1.5972\n",
      "Epoch-399, Time-0.3047, Train-MSE-0.0019, Train-L2-0.0200, Test-L2-1.5969\n",
      "Epoch-400, Time-0.3088, Train-MSE-0.0019, Train-L2-0.0200, Test-L2-1.5970\n",
      "Epoch-401, Time-0.3106, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5971\n",
      "Epoch-402, Time-0.3065, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5970\n",
      "Epoch-403, Time-0.3134, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5971\n",
      "Epoch-404, Time-0.2867, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5970\n",
      "Epoch-405, Time-0.3009, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5969\n",
      "Epoch-406, Time-0.3057, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5970\n",
      "Epoch-407, Time-0.3054, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5971\n",
      "Epoch-408, Time-0.3010, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5970\n",
      "Epoch-409, Time-0.3114, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5970\n",
      "Epoch-410, Time-0.3176, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5970\n",
      "Epoch-411, Time-0.3161, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5971\n",
      "Epoch-412, Time-0.3053, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5971\n",
      "Epoch-413, Time-0.3079, Train-MSE-0.0019, Train-L2-0.0199, Test-L2-1.5971\n",
      "Epoch-414, Time-0.3078, Train-MSE-0.0019, Train-L2-0.0198, Test-L2-1.5970\n",
      "Epoch-415, Time-0.3069, Train-MSE-0.0019, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-416, Time-0.3005, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-417, Time-0.3187, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-418, Time-0.3111, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5970\n",
      "Epoch-419, Time-0.3103, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-420, Time-0.3121, Train-MSE-0.0019, Train-L2-0.0198, Test-L2-1.5969\n",
      "Epoch-421, Time-0.3030, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-422, Time-0.3024, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5970\n",
      "Epoch-423, Time-0.3069, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-424, Time-0.3092, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5971\n",
      "Epoch-425, Time-0.3105, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5970\n",
      "Epoch-426, Time-0.3056, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5970\n",
      "Epoch-427, Time-0.2949, Train-MSE-0.0018, Train-L2-0.0198, Test-L2-1.5970\n",
      "Epoch-428, Time-0.3132, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-429, Time-0.3115, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-430, Time-0.2780, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-431, Time-0.3115, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5971\n",
      "Epoch-432, Time-0.2892, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-433, Time-0.3013, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5971\n",
      "Epoch-434, Time-0.2824, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5969\n",
      "Epoch-435, Time-0.2984, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-436, Time-0.2874, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-437, Time-0.2985, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5971\n",
      "Epoch-438, Time-0.2958, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-439, Time-0.2886, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-440, Time-0.2993, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-441, Time-0.2832, Train-MSE-0.0018, Train-L2-0.0197, Test-L2-1.5970\n",
      "Epoch-442, Time-0.2843, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-443, Time-0.2844, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5969\n",
      "Epoch-444, Time-0.2905, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-445, Time-0.3000, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-446, Time-0.2884, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-447, Time-0.2981, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-448, Time-0.2968, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5969\n",
      "Epoch-449, Time-0.3119, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5971\n",
      "Epoch-450, Time-0.3126, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-451, Time-0.2963, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-452, Time-0.3115, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-453, Time-0.3003, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-454, Time-0.2890, Train-MSE-0.0018, Train-L2-0.0196, Test-L2-1.5970\n",
      "Epoch-455, Time-0.3080, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-456, Time-0.3095, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-457, Time-0.3090, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-458, Time-0.3126, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-459, Time-0.3041, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-460, Time-0.2872, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-461, Time-0.3032, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5971\n",
      "Epoch-462, Time-0.3037, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-463, Time-0.3005, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-464, Time-0.3036, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-465, Time-0.3015, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-466, Time-0.3145, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5969\n",
      "Epoch-467, Time-0.2942, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-468, Time-0.3066, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5971\n",
      "Epoch-469, Time-0.3108, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5971\n",
      "Epoch-470, Time-0.3081, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-471, Time-0.3165, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-472, Time-0.3222, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5971\n",
      "Epoch-473, Time-0.2758, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5971\n",
      "Epoch-474, Time-0.2409, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-475, Time-0.2509, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5969\n",
      "Epoch-476, Time-0.2358, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-477, Time-0.2481, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5971\n",
      "Epoch-478, Time-0.2452, Train-MSE-0.0018, Train-L2-0.0195, Test-L2-1.5970\n",
      "Epoch-479, Time-0.2380, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-480, Time-0.2403, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5969\n",
      "Epoch-481, Time-0.2399, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5971\n",
      "Epoch-482, Time-0.2479, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5971\n",
      "Epoch-483, Time-0.2349, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5969\n",
      "Epoch-484, Time-0.2381, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-485, Time-0.2351, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-486, Time-0.2365, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-487, Time-0.2478, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-488, Time-0.2442, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-489, Time-0.2407, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-490, Time-0.2417, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-491, Time-0.2437, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-492, Time-0.2545, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-493, Time-0.2910, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-494, Time-0.2725, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-495, Time-0.2881, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-496, Time-0.2864, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-497, Time-0.2908, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-498, Time-0.2820, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n",
      "Epoch-499, Time-0.2765, Train-MSE-0.0018, Train-L2-0.0194, Test-L2-1.5970\n"
     ]
    }
   ],
   "source": [
    "myloss = LpLoss(size_average=False)\n",
    "for ep in range(epochs):\n",
    "    model_hf.train()\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader_hf:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model_hf(x)\n",
    "\n",
    "        mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "        l2.backward() # use the l2 relative loss\n",
    "\n",
    "        optimizer.step()\n",
    "        train_mse += mse.item()\n",
    "        train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    model_hf.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader_hf:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model_hf(x)\n",
    "            test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "\n",
    "    train_mse /= len(train_loader_hf)\n",
    "    train_l2 /= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print('Epoch-{}, Time-{:0.4f}, Train-MSE-{:0.4f}, Train-L2-{:0.4f}, Test-L2-{:0.4f}'\n",
    "          .format(ep, t2-t1, train_mse, train_l2, test_l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the HF-WNO model\n",
    "\n",
    "torch.save(model_hf, 'model/HF_WNO_poisson1D_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1658148072494,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "soCRGaXwS1MQ"
   },
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "pred_hf = []\n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    for x, y in test_loader_mf:\n",
    "        test_l2 = 0 \n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        out = model_hf(x[:,:,0:1]).squeeze(-1)\n",
    "        test_l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "        pred_hf.append( out.cpu() )\n",
    "        print(\"Batch-{}, Test-loss-{:0.6f}\".format( index, test_l2 ))\n",
    "        index += 1\n",
    "\n",
    "pred_hf = torch.cat(( pred_hf ))\n",
    "# print('Mean mse_mf-{}'.format(F.mse_loss(y_test_hf, pred_hf).item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1658148072495,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "D0X1lU-8S1MQ"
   },
   "outputs": [],
   "source": [
    "inp = x_test_mf\n",
    "real_hf = y_test_mf + inp[:,:,1] \n",
    "output_hf = pred_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_pred_hf = F.mse_loss(output_hf, real_hf).item()\n",
    "\n",
    "print('MSE-Predicted solution-{:0.4f}'.format(mse_pred_hf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1658148073691,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "SE7WNG9kS1MQ",
    "outputId": "5617df5c-b8cc-402a-c709-9c88bb99d2ca"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman' \n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "\n",
    "colormap = plt.cm.jet  \n",
    "colors = [colormap(i) for i in np.linspace(0, 1, 5)]\n",
    "\n",
    "fig2 = plt.figure(figsize = (10, 4), dpi=300)\n",
    "fig2.suptitle('Stochastic Heat - FNO - High fidelity')\n",
    "\n",
    "index = 0\n",
    "for i in range(ntest):\n",
    "    if i % 10 == 1:\n",
    "        plt.plot(x_coords, real_hf[i, :], color=colors[index], label='Actual')\n",
    "        plt.plot(x_coords, output_hf[i,:], '--', color=colors[index], label='Prediction')\n",
    "        index += 1\n",
    "plt.legend(ncol=4, loc=4, labelspacing=0.25, columnspacing=0.25, handletextpad=0.5, handlelength=1)\n",
    "plt.grid(True)\n",
    "plt.margins(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1658148076983,
     "user": {
      "displayName": "CSCCM IITD",
      "userId": "18000198353382878931"
     },
     "user_tz": -330
    },
    "id": "SmNhH6mheERy",
    "outputId": "7566fc00-3c85-417e-bf8f-0bc18f211886"
   },
   "outputs": [],
   "source": [
    "fig5, axs = plt.subplots(2, 2,figsize=(18,10), dpi=100)\n",
    "plt.subplots_adjust(wspace=0.25)\n",
    "\n",
    "n0 = 15\n",
    "axs[0, 0].plot(x_coords,real_mf[n0], linestyle='-', color='tab:green', lw=2)\n",
    "axs[0, 0].plot(x_coords,inp_mf[n0,:,0], linestyle=':', color='tab:blue', lw=2)\n",
    "axs[0, 0].plot(x_coords,output_hf[n0], linestyle='-.', color='tab:orange', lw=2)\n",
    "axs[0, 0].plot(x_coords,output_mf[n0], linestyle='--', color='tab:red', lw=3)\n",
    "axs[0, 0].legend(['HF-Truth','LF-WNO','HF-WNO','MF-WNO'])\n",
    "axs[0, 0].margins(0)\n",
    "axs[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "n1 = 28\n",
    "axs[0, 1].plot(x_coords,real_mf[n1], linestyle='-', color='tab:green', lw=2)\n",
    "axs[0, 1].plot(x_coords,inp_mf[n1,:,0], linestyle=':', color='tab:blue', lw=2)\n",
    "axs[0, 1].plot(x_coords,output_hf[n1], linestyle='-.', color='tab:orange', lw=2)\n",
    "axs[0, 1].plot(x_coords,output_mf[n1], linestyle='--', color='tab:red', lw=3)\n",
    "axs[0, 1].legend(['HF-Truth','LF-WNO','HF-WNO','MF-WNO'])\n",
    "axs[0, 1].margins(0)\n",
    "axs[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "n2 = 37\n",
    "axs[1, 0].plot(x_coords,real_mf[n2], linestyle='-', color='tab:green', lw=2)\n",
    "axs[1, 0].plot(x_coords,inp_mf[n2,:,0], linestyle=':', color='tab:blue', lw=2)\n",
    "axs[1, 0].plot(x_coords,output_hf[n2], linestyle='-.', color='tab:orange', lw=2)\n",
    "axs[1, 0].plot(x_coords,output_mf[n2], linestyle='--', color='tab:red', lw=3)\n",
    "axs[1, 0].legend(['HF-Truth','LF-WNO','HF-WNO','MF-WNO'])\n",
    "axs[1, 0].margins(0)\n",
    "axs[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "n3 = 38\n",
    "axs[1, 1].plot(x_coords,real_mf[n3], linestyle='-', color='tab:green', lw=2)\n",
    "axs[1, 1].plot(x_coords,inp_mf[n3,:,0], linestyle=':', color='tab:blue', lw=2)\n",
    "axs[1, 1].plot(x_coords,output_hf[n3], linestyle='-.', color='tab:orange', lw=2)\n",
    "axs[1, 1].plot(x_coords,output_mf[n3], linestyle='--', color='tab:red', lw=3)\n",
    "axs[1, 1].legend(['HF-Truth','LF-WNO','HF-WNO','MF-WNO'])\n",
    "axs[1, 1].margins(0)\n",
    "axs[1, 1].grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPnTmu8B9EPmJAx59FTLX1t",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Multifid_WNO_Poisson_new_size_25.ipynb",
   "provenance": [
    {
     "file_id": "1m6PDbFcTuqlZJGMznlUxbZWjYaQmyxcx",
     "timestamp": 1658047828483
    },
    {
     "file_id": "1Vl-BWLcc83HLihESRmzSzaa3xXaAo22L",
     "timestamp": 1658046554495
    },
    {
     "file_id": "1vdTJHNBQTe6cDa3znA2eK71i_o8rXg0m",
     "timestamp": 1657707104929
    },
    {
     "file_id": "1WmHRKITBoAIBvl_l46TQivFgEWBk8qbA",
     "timestamp": 1657553878903
    },
    {
     "file_id": "1c-ie-KRb-kGVgNiH1Ag-8WlV89iaMnAj",
     "timestamp": 1657551263885
    },
    {
     "file_id": "1zAzFWiQi2z4sAgFp0Fv7mYa0YowoFitv",
     "timestamp": 1657351594397
    },
    {
     "file_id": "1vodjDJ5vqUi0eylKTSABVpp7TinS4Zwx",
     "timestamp": 1657218736049
    },
    {
     "file_id": "1LgrC1t3iuy23p-b05fQh-ddyrsI0_lAv",
     "timestamp": 1657178812760
    },
    {
     "file_id": "1eSUGqtpAaymHy5y9I5bSv58S2X5G-MdX",
     "timestamp": 1657134232337
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
